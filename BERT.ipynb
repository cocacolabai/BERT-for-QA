{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import collections\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import optimization\n",
    "import six\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0418 16:45:15.261046 139979497596288 configuration_utils.py:185] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-config.json from cache at /home/jupyter/.cache/torch/transformers/8a3b1cfe5da58286e12a0f5d7d182b8d6eca88c08e26c332ee3817548cf7e60a.3767c74c8ed285531d04153fe84a0791672aff52f7249b27df341dbce09b8305\n",
      "I0418 16:45:15.262520 139979497596288 configuration_utils.py:199] Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": true,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "I0418 16:45:15.572837 139979497596288 modeling_utils.py:406] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-pytorch_model.bin from cache at /home/jupyter/.cache/torch/transformers/b1b5e295889f2d0979ede9a78ad9cb5dc6a0e25ab7f9417b315f0a2c22f4683d.929717ca66a3ba9eb9ec2f85973c6398c54c38a4faa464636a491d7a705f7eb6\n",
      "I0418 16:45:18.041263 139979497596288 tokenization_utils.py:398] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at /home/jupyter/.cache/torch/transformers/8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00\n"
     ]
    }
   ],
   "source": [
    "PRETRAINED_MODEL_NAME = \"bert-base-chinese\"  # 指定繁簡中文 BERT-BASE 預訓練模型\n",
    "model = BertModel.from_pretrained(PRETRAINED_MODEL_NAME, output_attentions=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "\n",
    "\n",
    "def main(_):\n",
    "  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n",
    "\n",
    "  bert_config = model.config\n",
    "\n",
    "  tf.io.gfile.makedirs(output_dir)\n",
    "  \n",
    "  \n",
    "  \n",
    "  tpu_cluster_resolver = None\n",
    "  if  use_tpu and  tpu_name:\n",
    "    tpu_cluster_resolver = tf.cluster_resolver.TPUClusterResolver(\n",
    "         tpu_name, zone= tpu_zone, project= gcp_project)\n",
    "\n",
    "  is_per_host = tf.compat.v1.estimator.tpu.InputPipelineConfig.PER_HOST_V2\n",
    "  run_config = tf.compat.v1.estimator.tpu.RunConfig(\n",
    "      cluster=tpu_cluster_resolver,\n",
    "      master= master,\n",
    "      model_dir= output_dir,\n",
    "      save_checkpoints_steps= save_checkpoints_steps,\n",
    "      tpu_config=tf.compat.v1.estimator.tpu.TPUConfig(\n",
    "          iterations_per_loop= iterations_per_loop,\n",
    "          num_shards= num_tpu_cores,\n",
    "          per_host_input_for_training=is_per_host))\n",
    "\n",
    "  train_examples = None\n",
    "  num_train_steps = None\n",
    "  num_warmup_steps = None\n",
    "  if  do_train:\n",
    "    train_examples = read_squad_examples(\n",
    "        input_file= train_file, is_training=True)\n",
    "    num_train_steps = int(\n",
    "        len(train_examples) /  train_batch_size *  num_train_epochs)\n",
    "    num_warmup_steps = int(num_train_steps *  warmup_proportion)\n",
    "\n",
    "    # Pre-shuffle the input to avoid having to make a very large shuffle\n",
    "    # buffer in in the `input_fn`.\n",
    "    rng = random.Random(12345)\n",
    "    rng.shuffle(train_examples)\n",
    "\n",
    "  model_fn = model_fn_builder(\n",
    "      init_checkpoint= init_checkpoint,\n",
    "      learning_rate= learning_rate,\n",
    "      num_train_steps=num_train_steps,\n",
    "      num_warmup_steps=num_warmup_steps,\n",
    "      use_tpu= use_tpu,\n",
    "      use_one_hot_embeddings= use_tpu)\n",
    "\n",
    "  # If TPU is not available, this will fall back to normal Estimator on CPU\n",
    "  # or GPU.\n",
    "  estimator = tf.compat.v1.estimator.tpu.TPUEstimator(\n",
    "      use_tpu= use_tpu,\n",
    "      model_fn=model_fn,\n",
    "      config=run_config,\n",
    "      train_batch_size= train_batch_size,\n",
    "      predict_batch_size= predict_batch_size)\n",
    "\n",
    "  if  do_train:\n",
    "    # We write to a temporary file to avoid storing very large constant tensors\n",
    "    # in memory.\n",
    "    train_writer = FeatureWriter(\n",
    "        filename=os.path.join( output_dir, \"train.tf_record\"),\n",
    "        is_training=True)\n",
    "    convert_examples_to_features(\n",
    "        examples=train_examples,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length= max_seq_length,\n",
    "        doc_stride= doc_stride,\n",
    "        max_query_length= max_query_length,\n",
    "        is_training=True,\n",
    "        output_fn=train_writer.process_feature)\n",
    "    train_writer.close()\n",
    "\n",
    "    tf.compat.v1.logging.info(\"***** Running training *****\")\n",
    "    tf.compat.v1.logging.info(\"  Num orig examples = %d\", len(train_examples))\n",
    "    tf.compat.v1.logging.info(\"  Num split examples = %d\", train_writer.num_features)\n",
    "    tf.compat.v1.logging.info(\"  Batch size = %d\",  train_batch_size)\n",
    "    tf.compat.v1.logging.info(\"  Num steps = %d\", num_train_steps)\n",
    "    del train_examples\n",
    "\n",
    "    train_input_fn = input_fn_builder(\n",
    "        input_file=train_writer.filename,\n",
    "        seq_length= max_seq_length,\n",
    "        is_training=True,\n",
    "        drop_remainder=True)\n",
    "    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "\n",
    "  if  do_predict:\n",
    "    eval_examples = read_squad_examples(\n",
    "        input_file= predict_file, is_training=False)\n",
    "\n",
    "    eval_writer = FeatureWriter(\n",
    "        filename=os.path.join( output_dir, \"eval.tf_record\"),\n",
    "        is_training=False)\n",
    "    eval_features = []\n",
    "\n",
    "    def append_feature(feature):\n",
    "      eval_features.append(feature)\n",
    "      eval_writer.process_feature(feature)\n",
    "\n",
    "    convert_examples_to_features(\n",
    "        examples=eval_examples,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length= max_seq_length,\n",
    "        doc_stride= doc_stride,\n",
    "        max_query_length= max_query_length,\n",
    "        is_training=False,\n",
    "        output_fn=append_feature)\n",
    "    eval_writer.close()\n",
    "\n",
    "    tf.compat.v1.logging.info(\"***** Running predictions *****\")\n",
    "    tf.compat.v1.logging.info(\"  Num orig examples = %d\", len(eval_examples))\n",
    "    tf.compat.v1.logging.info(\"  Num split examples = %d\", len(eval_features))\n",
    "    tf.compat.v1.logging.info(\"  Batch size = %d\",  predict_batch_size)\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    predict_input_fn = input_fn_builder(\n",
    "        input_file=eval_writer.filename,\n",
    "        seq_length= max_seq_length,\n",
    "        is_training=False,\n",
    "        drop_remainder=False)\n",
    "\n",
    "    # If running eval on the TPU, you will need to specify the number of\n",
    "    # steps.\n",
    "    all_results = []\n",
    "    for result in estimator.predict(\n",
    "        predict_input_fn, yield_single_examples=True):\n",
    "      if len(all_results) % 1000 == 0:\n",
    "        tf.compat.v1.logging.info(\"Processing example: %d\" % (len(all_results)))\n",
    "      unique_id = int(result[\"unique_ids\"])\n",
    "      start_logits = [float(x) for x in result[\"start_logits\"].flat]\n",
    "      end_logits = [float(x) for x in result[\"end_logits\"].flat]\n",
    "      all_results.append(\n",
    "          RawResult(\n",
    "              unique_id=unique_id,\n",
    "              start_logits=start_logits,\n",
    "              end_logits=end_logits))\n",
    "\n",
    "    output_prediction_file = os.path.join( output_dir, \"predictions.json\")\n",
    "    output_nbest_file = os.path.join( output_dir, \"nbest_predictions.json\")\n",
    "    output_null_log_odds_file = os.path.join( output_dir, \"null_odds.json\")\n",
    "\n",
    "    write_predictions(eval_examples, eval_features, all_results,\n",
    "                       n_best_size,  max_answer_length,\n",
    "                       do_lower_case, output_prediction_file,\n",
    "                      output_nbest_file, output_null_log_odds_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadExample(object):\n",
    "  \"\"\"A single training/test example for simple sequence classification.\n",
    "\n",
    "     For examples without an answer, the start and end position are -1.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               qas_id,\n",
    "               question_text,\n",
    "               doc_tokens,\n",
    "               orig_answer_text=None,\n",
    "               start_position=None,\n",
    "               end_position=None,\n",
    "               answerable=True):\n",
    "    self.qas_id = qas_id\n",
    "    self.question_text = question_text\n",
    "    self.doc_tokens = doc_tokens\n",
    "    self.orig_answer_text = orig_answer_text\n",
    "    self.start_position = start_position\n",
    "    self.end_position = end_position\n",
    "    self.answerable = answerable\n",
    "\n",
    "  def __str__(self):\n",
    "    return self.__repr__()\n",
    "\n",
    "  def __repr__(self):\n",
    "    s = \"\"\n",
    "    s += \"qas_id: %s\" % (printable_text(self.qas_id))\n",
    "    s += \", question_text: %s\" % (\n",
    "        printable_text(self.question_text))\n",
    "    s += \", doc_tokens: [%s]\" % (\" \".join(self.doc_tokens))\n",
    "    if self.start_position:\n",
    "      s += \", start_position: %d\" % (self.start_position)\n",
    "    if self.start_position:\n",
    "      s += \", end_position: %d\" % (self.end_position)\n",
    "    if self.start_position:\n",
    "      s += \", answerable: %r\" % (self.answerable)\n",
    "    return s\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "  \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               unique_id,\n",
    "               example_index,\n",
    "               doc_span_index,\n",
    "               tokens,\n",
    "               token_to_orig_map,\n",
    "               token_is_max_context,\n",
    "               input_ids,\n",
    "               input_mask,\n",
    "               segment_ids,\n",
    "               start_position=None,\n",
    "               end_position=None,\n",
    "               answerable=None):\n",
    "    self.unique_id = unique_id\n",
    "    self.example_index = example_index\n",
    "    self.doc_span_index = doc_span_index\n",
    "    self.tokens = tokens\n",
    "    self.token_to_orig_map = token_to_orig_map\n",
    "    self.token_is_max_context = token_is_max_context\n",
    "    self.input_ids = input_ids\n",
    "    self.input_mask = input_mask\n",
    "    self.segment_ids = segment_ids\n",
    "    self.start_position = start_position\n",
    "    self.end_position = end_position\n",
    "    self.answerable = answerable\n",
    "\n",
    "def whitespace_tokenize(text):\n",
    "  \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n",
    "  text = text.strip()\n",
    "  if not text:\n",
    "    return []\n",
    "  tokens = text.split()\n",
    "  return tokens\n",
    "\n",
    "def printable_text(text):\n",
    "  \"\"\"Returns text encoded in a way suitable for print or `tf.logging`.\"\"\"\n",
    "\n",
    "  # These functions want `str` for both Python2 and Python3, but in one case\n",
    "  # it's a Unicode string and in the other it's a byte string.\n",
    "  if six.PY3:\n",
    "    if isinstance(text, str):\n",
    "      return text\n",
    "    elif isinstance(text, bytes):\n",
    "      return text.decode(\"utf-8\", \"ignore\")\n",
    "    else:\n",
    "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "  elif six.PY2:\n",
    "    if isinstance(text, str):\n",
    "      return text\n",
    "    elif isinstance(text, unicode):\n",
    "      return text.encode(\"utf-8\")\n",
    "    else:\n",
    "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "  else:\n",
    "    raise ValueError(\"Not running on Python2 or Python 3?\")\n",
    "    \n",
    "    \n",
    "def read_squad_examples(input_file, is_training):\n",
    "  \"\"\"Read a SQuAD json file into a list of SquadExample.\"\"\"\n",
    "  with tf.compat.v1.gfile.Open(input_file, \"r\") as reader:\n",
    "    input_data = json.load(reader)[\"data\"]\n",
    "\n",
    "  def is_whitespace(c):\n",
    "    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F or ord(c) == 0x80:\n",
    "      return True\n",
    "    return False\n",
    "\n",
    "  examples = []\n",
    "    \n",
    "  for entry in input_data:\n",
    "    for paragraph in entry[\"paragraphs\"]:\n",
    "      paragraph_text = paragraph[\"context\"]\n",
    "\n",
    "      char_to_word_offset = []\n",
    "\n",
    "      doc_tokens = tokenizer.tokenize(paragraph_text)\n",
    "    \n",
    "      tokenIdx = 0\n",
    "      for token in doc_tokens:\n",
    "        tmp = token.replace(\" ##\", \"\")\n",
    "        tmp = tmp.replace(\"##\", \"\")\n",
    "        tmp = tmp.replace(\"[UNK]\", \" \")\n",
    "        l = len(tmp)\n",
    "        for i in range(l):\n",
    "            char_to_word_offset.append(tokenIdx)\n",
    "        tokenIdx+=1\n",
    "      \n",
    "#       print(doc_tokens)\n",
    "#       print(\"=============================\")\n",
    "#       print(paragraph_text)\n",
    "#       print(char_to_word_offset)\n",
    "      for qa in paragraph[\"qas\"]:\n",
    "        qas_id = qa[\"id\"]\n",
    "        question_text = qa[\"question\"]\n",
    "        start_position = None\n",
    "        end_position = None\n",
    "        orig_answer_text = None\n",
    "        answerable = True\n",
    "        if is_training:\n",
    "\n",
    "          if  version_2_with_negative:\n",
    "            answerable = qa[\"answerable\"]\n",
    "          if (len(qa[\"answers\"]) != 1) and answerable:\n",
    "            raise ValueError(\n",
    "                \"For training, each question should have exactly 1 answer.\")\n",
    "          if answerable:\n",
    "            answer = qa[\"answers\"][0]\n",
    "            orig_answer_text = answer[\"text\"]\n",
    "            answer_offset = answer[\"answer_start\"]\n",
    "            answer_length = len(orig_answer_text)\n",
    "            start_position = char_to_word_offset[answer_offset]\n",
    "            end_position = char_to_word_offset[answer_offset + answer_length - 1]\n",
    "\n",
    "            \n",
    "            # Only add answers where the text can be exactly recovered from the\n",
    "            # document. If this CAN'T happen it's likely due to weird Unicode\n",
    "            # stuff so we will just skip the example.\n",
    "            #\n",
    "            # Note that this means for training mode, every example is NOT\n",
    "            # guaranteed to be preserved.\n",
    "            actual_text = \"\".join(\n",
    "                doc_tokens[start_position:(end_position + 1)])\n",
    "#             cleaned_answer_text = \" \".join(whitespace_tokenize(orig_answer_text))\n",
    "\n",
    "            if actual_text.find(orig_answer_text) == -1:\n",
    "              tf.compat.v1.logging.warning(\"Could not find answer: '%s' vs. '%s'\",\n",
    "                                 actual_text, orig_answer_text)\n",
    "              continue\n",
    "          else:\n",
    "            start_position = -1\n",
    "            end_position = -1\n",
    "            orig_answer_text = \"\"\n",
    "\n",
    "        example = SquadExample(\n",
    "            qas_id=qas_id,\n",
    "            question_text=question_text,\n",
    "            doc_tokens=doc_tokens,\n",
    "            orig_answer_text=orig_answer_text,\n",
    "            start_position=start_position,\n",
    "            end_position=end_position,\n",
    "            answerable=answerable)\n",
    "        examples.append(example)\n",
    "  return examples\n",
    "\n",
    "\n",
    "def convert_examples_to_features(examples, tokenizer, max_seq_length,\n",
    "                                 doc_stride, max_query_length, is_training,\n",
    "                                 output_fn):\n",
    "  \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "  unique_id = 1000000000\n",
    "\n",
    "  for (example_index, example) in enumerate(examples):\n",
    "    query_tokens = tokenizer.tokenize(example.question_text)\n",
    "\n",
    "    if len(query_tokens) > max_query_length:\n",
    "      query_tokens = query_tokens[0:max_query_length]\n",
    "\n",
    "    tok_to_orig_index = []\n",
    "    orig_to_tok_index = []\n",
    "    all_doc_tokens = []\n",
    "    for (i, token) in enumerate(example.doc_tokens):\n",
    "      orig_to_tok_index.append(len(all_doc_tokens))\n",
    "      sub_tokens = tokenizer.tokenize(token)\n",
    "      for sub_token in sub_tokens:\n",
    "        tok_to_orig_index.append(i)\n",
    "        all_doc_tokens.append(sub_token)\n",
    "\n",
    "    tok_start_position = None\n",
    "    tok_end_position = None\n",
    "    if is_training and not example.answerable:\n",
    "      tok_start_position = -1\n",
    "      tok_end_position = -1\n",
    "    if is_training and example.answerable:\n",
    "      tok_start_position = orig_to_tok_index[example.start_position]\n",
    "      if example.end_position < len(example.doc_tokens) - 1:\n",
    "        tok_end_position = orig_to_tok_index[example.end_position + 1] - 1\n",
    "      else:\n",
    "        tok_end_position = len(all_doc_tokens) - 1\n",
    "      (tok_start_position, tok_end_position) = _improve_answer_span(\n",
    "          all_doc_tokens, tok_start_position, tok_end_position, tokenizer,\n",
    "          example.orig_answer_text)\n",
    "\n",
    "    # The -3 accounts for [CLS], [SEP] and [SEP]\n",
    "    max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n",
    "\n",
    "    # We can have documents that are longer than the maximum sequence length.\n",
    "    # To deal with this we do a sliding window approach, where we take chunks\n",
    "    # of the up to our max length with a stride of `doc_stride`.\n",
    "    _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "        \"DocSpan\", [\"start\", \"length\"])\n",
    "    doc_spans = []\n",
    "    start_offset = 0\n",
    "    while start_offset < len(all_doc_tokens):\n",
    "      length = len(all_doc_tokens) - start_offset\n",
    "      if length > max_tokens_for_doc:\n",
    "        length = max_tokens_for_doc\n",
    "      doc_spans.append(_DocSpan(start=start_offset, length=length))\n",
    "      if start_offset + length == len(all_doc_tokens):\n",
    "        break\n",
    "      start_offset += min(length, doc_stride)\n",
    "\n",
    "    for (doc_span_index, doc_span) in enumerate(doc_spans):\n",
    "      tokens = []\n",
    "      token_to_orig_map = {}\n",
    "      token_is_max_context = {}\n",
    "      segment_ids = []\n",
    "      tokens.append(\"[CLS]\")\n",
    "      segment_ids.append(0)\n",
    "      for token in query_tokens:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "      tokens.append(\"[SEP]\")\n",
    "      segment_ids.append(0)\n",
    "\n",
    "      for i in range(doc_span.length):\n",
    "        split_token_index = doc_span.start + i\n",
    "        token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n",
    "\n",
    "        is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n",
    "                                               split_token_index)\n",
    "        token_is_max_context[len(tokens)] = is_max_context\n",
    "        tokens.append(all_doc_tokens[split_token_index])\n",
    "        segment_ids.append(1)\n",
    "      tokens.append(\"[SEP]\")\n",
    "      segment_ids.append(1)\n",
    "\n",
    "      input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "      # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "      # tokens are attended to.\n",
    "      input_mask = [1] * len(input_ids)\n",
    "\n",
    "      # Zero-pad up to the sequence length.\n",
    "      while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "\n",
    "      assert len(input_ids) == max_seq_length\n",
    "      assert len(input_mask) == max_seq_length\n",
    "      assert len(segment_ids) == max_seq_length\n",
    "\n",
    "      start_position = None\n",
    "      end_position = None\n",
    "      if is_training and example.answerable:\n",
    "        # For training, if our document chunk does not contain an annotation\n",
    "        # we throw it out, since there is nothing to predict.\n",
    "        doc_start = doc_span.start\n",
    "        doc_end = doc_span.start + doc_span.length - 1\n",
    "        out_of_span = False\n",
    "        if not (tok_start_position >= doc_start and\n",
    "                tok_end_position <= doc_end):\n",
    "          out_of_span = True\n",
    "        if out_of_span:\n",
    "          start_position = 0\n",
    "          end_position = 0\n",
    "        else:\n",
    "          doc_offset = len(query_tokens) + 2\n",
    "          start_position = tok_start_position - doc_start + doc_offset\n",
    "          end_position = tok_end_position - doc_start + doc_offset\n",
    "\n",
    "      if is_training and not example.answerable:\n",
    "        start_position = 0\n",
    "        end_position = 0\n",
    "\n",
    "      if example_index < 20:\n",
    "        tf.compat.v1.logging.info(\"*** Example ***\")\n",
    "        tf.compat.v1.logging.info(\"unique_id: %s\" % (unique_id))\n",
    "        tf.compat.v1.logging.info(\"example_index: %s\" % (example_index))\n",
    "#         tf.compat.v1.logging.info(\"doc_span_index: %s\" % (doc_span_index))\n",
    "#         tf.compat.v1.logging.info(\"tokens: %s\" % \" \".join(\n",
    "#             [BertTokenizer.printable_text(x) for x in tokens]))\n",
    "#         tf.compat.v1.logging.info(\"token_to_orig_map: %s\" % \" \".join(\n",
    "#             [\"%d:%d\" % (x, y) for (x, y) in six.iteritems(token_to_orig_map)]))\n",
    "#         tf.compat.v1.logging.info(\"token_is_max_context: %s\" % \" \".join([\n",
    "#             \"%d:%s\" % (x, y) for (x, y) in six.iteritems(token_is_max_context)\n",
    "#         ]))\n",
    "#         tf.compat.v1.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "#         tf.compat.v1.logging.info(\n",
    "#             \"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "#         tf.compat.v1.logging.info(\n",
    "#             \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "        if is_training and not example.answerable:\n",
    "          tf.compat.v1.logging.info(\"impossible example\")\n",
    "        if is_training and example.answerable:\n",
    "          answer_text = \" \".join(tokens[start_position:(end_position + 1)])\n",
    "          tf.compat.v1.logging.info(\"start_position: %d\" % (start_position))\n",
    "          tf.compat.v1.logging.info(\"end_position: %d\" % (end_position))\n",
    "          tf.compat.v1.logging.info(\n",
    "              \"answer: %s\" % (printable_text(answer_text)))\n",
    "\n",
    "      feature = InputFeatures(\n",
    "          unique_id=unique_id,\n",
    "          example_index=example_index,\n",
    "          doc_span_index=doc_span_index,\n",
    "          tokens=tokens,\n",
    "          token_to_orig_map=token_to_orig_map,\n",
    "          token_is_max_context=token_is_max_context,\n",
    "          input_ids=input_ids,\n",
    "          input_mask=input_mask,\n",
    "          segment_ids=segment_ids,\n",
    "          start_position=start_position,\n",
    "          end_position=end_position,\n",
    "          answerable=example.answerable)\n",
    "\n",
    "      # Run callback\n",
    "      output_fn(feature)\n",
    "\n",
    "      unique_id += 1\n",
    "\n",
    "\n",
    "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n",
    "                         orig_answer_text):\n",
    "  \"\"\"Returns tokenized answer spans that better match the annotated answer.\"\"\"\n",
    "\n",
    "  # The SQuAD annotations are character based. We first project them to\n",
    "  # whitespace-tokenized words. But then after WordPiece BertTokenizer, we can\n",
    "  # often find a \"better match\". For example:\n",
    "  #\n",
    "  #   Question: What year was John Smith born?\n",
    "  #   Context: The leader was John Smith (1895-1943).\n",
    "  #   Answer: 1895\n",
    "  #\n",
    "  # The original whitespace-tokenized answer will be \"(1895-1943).\". However\n",
    "  # after BertTokenizer, our tokens will be \"( 1895 - 1943 ) .\". So we can match\n",
    "  # the exact answer, 1895.\n",
    "  #\n",
    "  # However, this is not always possible. Consider the following:\n",
    "  #\n",
    "  #   Question: What country is the top exporter of electornics?\n",
    "  #   Context: The Japanese electronics industry is the lagest in the world.\n",
    "  #   Answer: Japan\n",
    "  #\n",
    "  # In this case, the annotator chose \"Japan\" as a character sub-span of\n",
    "  # the word \"Japanese\". Since our WordPiece tokenizer does not split\n",
    "  # \"Japanese\", we just use \"Japanese\" as the annotation. This is fairly rare\n",
    "  # in SQuAD, but does happen.\n",
    "  tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n",
    "\n",
    "  for new_start in range(input_start, input_end + 1):\n",
    "    for new_end in range(input_end, new_start - 1, -1):\n",
    "      text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n",
    "      if text_span == tok_answer_text:\n",
    "        return (new_start, new_end)\n",
    "\n",
    "  return (input_start, input_end)\n",
    "\n",
    "\n",
    "def _check_is_max_context(doc_spans, cur_span_index, position):\n",
    "  \"\"\"Check if this is the 'max context' doc span for the token.\"\"\"\n",
    "\n",
    "  # Because of the sliding window approach taken to scoring documents, a single\n",
    "  # token can appear in multiple documents. E.g.\n",
    "  #  Doc: the man went to the store and bought a gallon of milk\n",
    "  #  Span A: the man went to the\n",
    "  #  Span B: to the store and bought\n",
    "  #  Span C: and bought a gallon of\n",
    "  #  ...\n",
    "  #\n",
    "  # Now the word 'bought' will have two scores from spans B and C. We only\n",
    "  # want to consider the score with \"maximum context\", which we define as\n",
    "  # the *minimum* of its left and right context (the *sum* of left and\n",
    "  # right context will always be the same, of course).\n",
    "  #\n",
    "  # In the example the maximum context for 'bought' would be span C since\n",
    "  # it has 1 left context and 3 right context, while span B has 4 left context\n",
    "  # and 0 right context.\n",
    "  best_score = None\n",
    "  best_span_index = None\n",
    "  for (span_index, doc_span) in enumerate(doc_spans):\n",
    "    end = doc_span.start + doc_span.length - 1\n",
    "    if position < doc_span.start:\n",
    "      continue\n",
    "    if position > end:\n",
    "      continue\n",
    "    num_left_context = position - doc_span.start\n",
    "    num_right_context = end - position\n",
    "    score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n",
    "    if best_score is None or score > best_score:\n",
    "      best_score = score\n",
    "      best_span_index = span_index\n",
    "\n",
    "  return cur_span_index == best_span_index\n",
    "\n",
    "\n",
    "def create_model(is_training, input_ids, segment_ids,\n",
    "                 use_one_hot_embeddings):\n",
    "  \"\"\"Creates a classification model.\"\"\"\n",
    "\n",
    "  \n",
    "\n",
    "  final_hidden = model.get_sequence_output()\n",
    "\n",
    "  final_hidden_shape = BertModel.get_shape_list(final_hidden, expected_rank=3)\n",
    "  batch_size = final_hidden_shape[0]\n",
    "  seq_length = final_hidden_shape[1]\n",
    "  hidden_size = final_hidden_shape[2]\n",
    "\n",
    "  output_weights = tf.get_variable(\n",
    "      \"cls/squad/output_weights\", [2, hidden_size],\n",
    "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "  output_bias = tf.get_variable(\n",
    "      \"cls/squad/output_bias\", [2], initializer=tf.zeros_initializer())\n",
    "\n",
    "  final_hidden_matrix = tf.reshape(final_hidden,\n",
    "                                   [batch_size * seq_length, hidden_size])\n",
    "  logits = tf.matmul(final_hidden_matrix, output_weights, transpose_b=True)\n",
    "  logits = tf.nn.bias_add(logits, output_bias)\n",
    "\n",
    "  logits = tf.reshape(logits, [batch_size, seq_length, 2])\n",
    "  logits = tf.transpose(logits, [2, 0, 1])\n",
    "\n",
    "  unstacked_logits = tf.unstack(logits, axis=0)\n",
    "\n",
    "  (start_logits, end_logits) = (unstacked_logits[0], unstacked_logits[1])\n",
    "\n",
    "  return (start_logits, end_logits)\n",
    "\n",
    "\n",
    "def model_fn_builder(init_checkpoint, learning_rate,\n",
    "                     num_train_steps, num_warmup_steps, use_tpu,\n",
    "                     use_one_hot_embeddings):\n",
    "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "\n",
    "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "\n",
    "    tf.compat.v1.logging.info(\"*** Features ***\")\n",
    "    for name in sorted(features.keys()):\n",
    "      tf.compat.v1.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
    "\n",
    "    unique_ids = features[\"unique_ids\"]\n",
    "    input_ids = features[\"input_ids\"]\n",
    "    input_mask = features[\"input_mask\"]\n",
    "    segment_ids = features[\"segment_ids\"]\n",
    "\n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    (start_logits, end_logits) = create_model(\n",
    "        is_training=is_training,\n",
    "        input_ids=input_ids,\n",
    "        segment_ids=segment_ids,\n",
    "        use_one_hot_embeddings=use_one_hot_embeddings)\n",
    "\n",
    "    tvars = tf.compat.v1.trainable_variables()\n",
    "\n",
    "    initialized_variable_names = {}\n",
    "    scaffold_fn = None\n",
    "    if init_checkpoint:\n",
    "      (assignment_map, initialized_variable_names\n",
    "      ) = BertModel.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
    "      if use_tpu:\n",
    "\n",
    "        def tpu_scaffold():\n",
    "          tf.compat.v1.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "          return tf.compat.v1.train.Scaffold()\n",
    "\n",
    "        scaffold_fn = tpu_scaffold\n",
    "      else:\n",
    "        tf.compat.v1.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "\n",
    "    tf.compat.v1.logging.info(\"**** Trainable Variables ****\")\n",
    "    for var in tvars:\n",
    "      init_string = \"\"\n",
    "      if var.name in initialized_variable_names:\n",
    "        init_string = \", *INIT_FROM_CKPT*\"\n",
    "      tf.compat.v1.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n",
    "                      init_string)\n",
    "\n",
    "    output_spec = None\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "      seq_length = BertModel.get_shape_list(input_ids)[1]\n",
    "\n",
    "      def compute_loss(logits, positions):\n",
    "        one_hot_positions = tf.one_hot(\n",
    "            positions, depth=seq_length, dtype=tf.float32)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "        loss = -tf.reduce_mean(\n",
    "            tf.reduce_sum(one_hot_positions * log_probs, axis=-1))\n",
    "        return loss\n",
    "\n",
    "      start_positions = features[\"start_positions\"]\n",
    "      end_positions = features[\"end_positions\"]\n",
    "\n",
    "      start_loss = compute_loss(start_logits, start_positions)\n",
    "      end_loss = compute_loss(end_logits, end_positions)\n",
    "\n",
    "      total_loss = (start_loss + end_loss) / 2.0\n",
    "\n",
    "      train_op = optimization.create_optimizer(\n",
    "          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n",
    "\n",
    "      output_spec = tf.compat.v1.estimator.tpu.TPUEstimatorSpec(\n",
    "          mode=mode,\n",
    "          loss=total_loss,\n",
    "          train_op=train_op,\n",
    "          scaffold_fn=scaffold_fn)\n",
    "    elif mode == tf.estimator.ModeKeys.PREDICT:\n",
    "      predictions = {\n",
    "          \"unique_ids\": unique_ids,\n",
    "          \"start_logits\": start_logits,\n",
    "          \"end_logits\": end_logits,\n",
    "      }\n",
    "      output_spec = tf.compat.v1.estimator.tpu.TPUEstimatorSpec(\n",
    "          mode=mode, predictions=predictions, scaffold_fn=scaffold_fn)\n",
    "    else:\n",
    "      raise ValueError(\n",
    "          \"Only TRAIN and PREDICT modes are supported: %s\" % (mode))\n",
    "\n",
    "    return output_spec\n",
    "\n",
    "  return model_fn\n",
    "\n",
    "\n",
    "def input_fn_builder(input_file, seq_length, is_training, drop_remainder):\n",
    "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
    "\n",
    "  name_to_features = {\n",
    "      \"unique_ids\": tf.io.FixedLenFeature([], tf.int64),\n",
    "      \"input_ids\": tf.io.FixedLenFeature([seq_length], tf.int64),\n",
    "      \"input_mask\": tf.io.FixedLenFeature([seq_length], tf.int64),\n",
    "      \"segment_ids\": tf.io.FixedLenFeature([seq_length], tf.int64),\n",
    "  }\n",
    "\n",
    "  if is_training:\n",
    "    name_to_features[\"start_positions\"] = tf.io.FixedLenFeature([], tf.int64)\n",
    "    name_to_features[\"end_positions\"] = tf.io.FixedLenFeature([], tf.int64)\n",
    "\n",
    "  def _decode_record(record, name_to_features):\n",
    "    \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "    example = tf.io.parse_single_example(record, name_to_features)\n",
    "\n",
    "    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
    "    # So cast all int64 to int32.\n",
    "    for name in list(example.keys()):\n",
    "      t = example[name]\n",
    "      if t.dtype == tf.int64:\n",
    "        t = tf.compat.v1.to_int32(t)\n",
    "      example[name] = t\n",
    "\n",
    "    return example\n",
    "\n",
    "  def input_fn(params):\n",
    "    \"\"\"The actual input function.\"\"\"\n",
    "    batch_size = params[\"batch_size\"]\n",
    "\n",
    "    # For training, we want a lot of parallel reading and shuffling.\n",
    "    # For eval, we want no shuffling and parallel reading doesn't matter.\n",
    "    d = tf.data.TFRecordDataset(input_file)\n",
    "    if is_training:\n",
    "      d = d.repeat()\n",
    "      d = d.shuffle(buffer_size=100)\n",
    "\n",
    "    d = d.apply(\n",
    "        tf.data.experimental.map_and_batch(\n",
    "            lambda record: _decode_record(record, name_to_features),\n",
    "            batch_size=batch_size,\n",
    "            drop_remainder=drop_remainder))\n",
    "\n",
    "    return d\n",
    "\n",
    "  return input_fn\n",
    "\n",
    "\n",
    "RawResult = collections.namedtuple(\"RawResult\",\n",
    "                                   [\"unique_id\", \"start_logits\", \"end_logits\"])\n",
    "\n",
    "\n",
    "def write_predictions(all_examples, all_features, all_results, n_best_size,\n",
    "                      max_answer_length, do_lower_case, output_prediction_file,\n",
    "                      output_nbest_file, output_null_log_odds_file):\n",
    "  \"\"\"Write final predictions to the json file and log-odds of null if needed.\"\"\"\n",
    "  tf.compat.v1.logging.info(\"Writing predictions to: %s\" % (output_prediction_file))\n",
    "  tf.compat.v1.logging.info(\"Writing nbest to: %s\" % (output_nbest_file))\n",
    "\n",
    "  example_index_to_features = collections.defaultdict(list)\n",
    "  for feature in all_features:\n",
    "    example_index_to_features[feature.example_index].append(feature)\n",
    "\n",
    "  unique_id_to_result = {}\n",
    "  for result in all_results:\n",
    "    unique_id_to_result[result.unique_id] = result\n",
    "\n",
    "  _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "      \"PrelimPrediction\",\n",
    "      [\"feature_index\", \"start_index\", \"end_index\", \"start_logit\", \"end_logit\"])\n",
    "\n",
    "  all_predictions = collections.OrderedDict()\n",
    "  all_nbest_json = collections.OrderedDict()\n",
    "  scores_diff_json = collections.OrderedDict()\n",
    "\n",
    "  for (example_index, example) in enumerate(all_examples):\n",
    "    features = example_index_to_features[example_index]\n",
    "\n",
    "    prelim_predictions = []\n",
    "    # keep track of the minimum score of null start+end of position 0\n",
    "    score_null = 1000000  # large and positive\n",
    "    min_null_feature_index = 0  # the paragraph slice with min mull score\n",
    "    null_start_logit = 0  # the start logit at the slice with min null score\n",
    "    null_end_logit = 0  # the end logit at the slice with min null score\n",
    "    for (feature_index, feature) in enumerate(features):\n",
    "      result = unique_id_to_result[feature.unique_id]\n",
    "      start_indexes = _get_best_indexes(result.start_logits, n_best_size)\n",
    "      end_indexes = _get_best_indexes(result.end_logits, n_best_size)\n",
    "      # if we could have irrelevant answers, get the min score of irrelevant\n",
    "      if  version_2_with_negative:\n",
    "        feature_null_score = result.start_logits[0] + result.end_logits[0]\n",
    "        if feature_null_score < score_null:\n",
    "          score_null = feature_null_score\n",
    "          min_null_feature_index = feature_index\n",
    "          null_start_logit = result.start_logits[0]\n",
    "          null_end_logit = result.end_logits[0]\n",
    "      for start_index in start_indexes:\n",
    "        for end_index in end_indexes:\n",
    "          # We could hypothetically create invalid predictions, e.g., predict\n",
    "          # that the start of the span is in the question. We throw out all\n",
    "          # invalid predictions.\n",
    "          if start_index >= len(feature.tokens):\n",
    "            continue\n",
    "          if end_index >= len(feature.tokens):\n",
    "            continue\n",
    "          if start_index not in feature.token_to_orig_map:\n",
    "            continue\n",
    "          if end_index not in feature.token_to_orig_map:\n",
    "            continue\n",
    "          if not feature.token_is_max_context.get(start_index, False):\n",
    "            continue\n",
    "          if end_index < start_index:\n",
    "            continue\n",
    "          length = end_index - start_index + 1\n",
    "          if length > max_answer_length:\n",
    "            continue\n",
    "          prelim_predictions.append(\n",
    "              _PrelimPrediction(\n",
    "                  feature_index=feature_index,\n",
    "                  start_index=start_index,\n",
    "                  end_index=end_index,\n",
    "                  start_logit=result.start_logits[start_index],\n",
    "                  end_logit=result.end_logits[end_index]))\n",
    "\n",
    "    if  version_2_with_negative:\n",
    "      prelim_predictions.append(\n",
    "          _PrelimPrediction(\n",
    "              feature_index=min_null_feature_index,\n",
    "              start_index=0,\n",
    "              end_index=0,\n",
    "              start_logit=null_start_logit,\n",
    "              end_logit=null_end_logit))\n",
    "    prelim_predictions = sorted(\n",
    "        prelim_predictions,\n",
    "        key=lambda x: (x.start_logit + x.end_logit),\n",
    "        reverse=True)\n",
    "\n",
    "    _NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "        \"NbestPrediction\", [\"text\", \"start_logit\", \"end_logit\"])\n",
    "\n",
    "    seen_predictions = {}\n",
    "    nbest = []\n",
    "    for pred in prelim_predictions:\n",
    "      if len(nbest) >= n_best_size:\n",
    "        break\n",
    "      feature = features[pred.feature_index]\n",
    "      if pred.start_index > 0:  # this is a non-null prediction\n",
    "        tok_tokens = feature.tokens[pred.start_index:(pred.end_index + 1)]\n",
    "        orig_doc_start = feature.token_to_orig_map[pred.start_index]\n",
    "        orig_doc_end = feature.token_to_orig_map[pred.end_index]\n",
    "        orig_tokens = example.doc_tokens[orig_doc_start:(orig_doc_end + 1)]\n",
    "        tok_text = \" \".join(tok_tokens)\n",
    "\n",
    "        # De-tokenize WordPieces that have been split off.\n",
    "        tok_text = tok_text.replace(\" ##\", \"\")\n",
    "        tok_text = tok_text.replace(\"##\", \"\")\n",
    "\n",
    "        # Clean whitespace\n",
    "        tok_text = tok_text.strip()\n",
    "        tok_text = \" \".join(tok_text.split())\n",
    "        orig_text = \" \".join(orig_tokens)\n",
    "\n",
    "        final_text = get_final_text(tok_text, orig_text, do_lower_case)\n",
    "        if final_text in seen_predictions:\n",
    "          continue\n",
    "\n",
    "        seen_predictions[final_text] = True\n",
    "      else:\n",
    "        final_text = \"\"\n",
    "        seen_predictions[final_text] = True\n",
    "\n",
    "      nbest.append(\n",
    "          _NbestPrediction(\n",
    "              text=final_text,\n",
    "              start_logit=pred.start_logit,\n",
    "              end_logit=pred.end_logit))\n",
    "\n",
    "    # if we didn't inlude the empty option in the n-best, inlcude it\n",
    "    if  version_2_with_negative:\n",
    "      if \"\" not in seen_predictions:\n",
    "        nbest.append(\n",
    "            _NbestPrediction(\n",
    "                text=\"\", start_logit=null_start_logit,\n",
    "                end_logit=null_end_logit))\n",
    "    # In very rare edge cases we could have no valid predictions. So we\n",
    "    # just create a nonce prediction in this case to avoid failure.\n",
    "    if not nbest:\n",
    "      nbest.append(\n",
    "          _NbestPrediction(text=\"empty\", start_logit=0.0, end_logit=0.0))\n",
    "\n",
    "    assert len(nbest) >= 1\n",
    "\n",
    "    total_scores = []\n",
    "    best_non_null_entry = None\n",
    "    for entry in nbest:\n",
    "      total_scores.append(entry.start_logit + entry.end_logit)\n",
    "      if not best_non_null_entry:\n",
    "        if entry.text:\n",
    "          best_non_null_entry = entry\n",
    "\n",
    "    probs = _compute_softmax(total_scores)\n",
    "\n",
    "    nbest_json = []\n",
    "    for (i, entry) in enumerate(nbest):\n",
    "      output = collections.OrderedDict()\n",
    "      output[\"text\"] = entry.text\n",
    "      output[\"probability\"] = probs[i]\n",
    "      output[\"start_logit\"] = entry.start_logit\n",
    "      output[\"end_logit\"] = entry.end_logit\n",
    "      nbest_json.append(output)\n",
    "\n",
    "    assert len(nbest_json) >= 1\n",
    "\n",
    "    if not  version_2_with_negative:\n",
    "      all_predictions[example.qas_id] = nbest_json[0][\"text\"]\n",
    "    else:\n",
    "      # predict \"\" iff the null score - the score of best non-null > threshold\n",
    "      score_diff = score_null - best_non_null_entry.start_logit - (\n",
    "          best_non_null_entry.end_logit)\n",
    "      scores_diff_json[example.qas_id] = score_diff\n",
    "      if score_diff >  null_score_diff_threshold:\n",
    "        all_predictions[example.qas_id] = \"\"\n",
    "      else:\n",
    "        all_predictions[example.qas_id] = best_non_null_entry.text\n",
    "\n",
    "    all_nbest_json[example.qas_id] = nbest_json\n",
    "\n",
    "  with tf.io.gfile.GFile(output_prediction_file, \"w\") as writer:\n",
    "    writer.write(json.dumps(all_predictions, indent=4) + \"\\n\")\n",
    "\n",
    "  with tf.io.gfile.GFile(output_nbest_file, \"w\") as writer:\n",
    "    writer.write(json.dumps(all_nbest_json, indent=4) + \"\\n\")\n",
    "\n",
    "  if  version_2_with_negative:\n",
    "    with tf.io.gfile.GFile(output_null_log_odds_file, \"w\") as writer:\n",
    "      writer.write(json.dumps(scores_diff_json, indent=4) + \"\\n\")\n",
    "\n",
    "\n",
    "def get_final_text(pred_text, orig_text, do_lower_case):\n",
    "  \"\"\"Project the tokenized prediction back to the original text.\"\"\"\n",
    "\n",
    "  # When we created the data, we kept track of the alignment between original\n",
    "  # (whitespace tokenized) tokens and our WordPiece tokenized tokens. So\n",
    "  # now `orig_text` contains the span of our original text corresponding to the\n",
    "  # span that we predicted.\n",
    "  #\n",
    "  # However, `orig_text` may contain extra characters that we don't want in\n",
    "  # our prediction.\n",
    "  #\n",
    "  # For example, let's say:\n",
    "  #   pred_text = steve smith\n",
    "  #   orig_text = Steve Smith's\n",
    "  #\n",
    "  # We don't want to return `orig_text` because it contains the extra \"'s\".\n",
    "  #\n",
    "  # We don't want to return `pred_text` because it's already been normalized\n",
    "  # (the SQuAD eval script also does punctuation stripping/lower casing but\n",
    "  # our tokenizer does additional normalization like stripping accent\n",
    "  # characters).\n",
    "  #\n",
    "  # What we really want to return is \"Steve Smith\".\n",
    "  #\n",
    "  # Therefore, we have to apply a semi-complicated alignment heruistic between\n",
    "  # `pred_text` and `orig_text` to get a character-to-charcter alignment. This\n",
    "  # can fail in certain cases in which case we just return `orig_text`.\n",
    "\n",
    "  def _strip_spaces(text):\n",
    "    ns_chars = []\n",
    "    ns_to_s_map = collections.OrderedDict()\n",
    "    for (i, c) in enumerate(text):\n",
    "      if c == \" \":\n",
    "        continue\n",
    "      ns_to_s_map[len(ns_chars)] = i\n",
    "      ns_chars.append(c)\n",
    "    ns_text = \"\".join(ns_chars)\n",
    "    return (ns_text, ns_to_s_map)\n",
    "\n",
    "  # We first tokenize `orig_text`, strip whitespace from the result\n",
    "  # and `pred_text`, and check if they are the same length. If they are\n",
    "  # NOT the same length, the heuristic has failed. If they are the same\n",
    "  # length, we assume the characters are one-to-one aligned.\n",
    "\n",
    "  tok_text = \" \".join(tokenizer.tokenize(orig_text))\n",
    "\n",
    "  start_position = tok_text.find(pred_text)\n",
    "  if start_position == -1:\n",
    "    if  verbose_logging:\n",
    "      tf.compat.v1.logging.info(\n",
    "          \"Unable to find text: '%s' in '%s'\" % (pred_text, orig_text))\n",
    "    return orig_text\n",
    "  end_position = start_position + len(pred_text) - 1\n",
    "\n",
    "  (orig_ns_text, orig_ns_to_s_map) = _strip_spaces(orig_text)\n",
    "  (tok_ns_text, tok_ns_to_s_map) = _strip_spaces(tok_text)\n",
    "\n",
    "  if len(orig_ns_text) != len(tok_ns_text):\n",
    "    if  verbose_logging:\n",
    "      tf.compat.v1.logging.info(\"Length not equal after stripping spaces: '%s' vs '%s'\",\n",
    "                      orig_ns_text, tok_ns_text)\n",
    "    return orig_text\n",
    "\n",
    "  # We then project the characters in `pred_text` back to `orig_text` using\n",
    "  # the character-to-character alignment.\n",
    "  tok_s_to_ns_map = {}\n",
    "  for (i, tok_index) in six.iteritems(tok_ns_to_s_map):\n",
    "    tok_s_to_ns_map[tok_index] = i\n",
    "\n",
    "  orig_start_position = None\n",
    "  if start_position in tok_s_to_ns_map:\n",
    "    ns_start_position = tok_s_to_ns_map[start_position]\n",
    "    if ns_start_position in orig_ns_to_s_map:\n",
    "      orig_start_position = orig_ns_to_s_map[ns_start_position]\n",
    "\n",
    "  if orig_start_position is None:\n",
    "    if  verbose_logging:\n",
    "      tf.compat.v1.logging.info(\"Couldn't map start position\")\n",
    "    return orig_text\n",
    "\n",
    "  orig_end_position = None\n",
    "  if end_position in tok_s_to_ns_map:\n",
    "    ns_end_position = tok_s_to_ns_map[end_position]\n",
    "    if ns_end_position in orig_ns_to_s_map:\n",
    "      orig_end_position = orig_ns_to_s_map[ns_end_position]\n",
    "\n",
    "  if orig_end_position is None:\n",
    "    if  verbose_logging:\n",
    "      tf.compat.v1.logging.info(\"Couldn't map end position\")\n",
    "    return orig_text\n",
    "\n",
    "  output_text = orig_text[orig_start_position:(orig_end_position + 1)]\n",
    "  return output_text\n",
    "\n",
    "\n",
    "def _get_best_indexes(logits, n_best_size):\n",
    "  \"\"\"Get the n-best logits from a list.\"\"\"\n",
    "  index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "  best_indexes = []\n",
    "  for i in range(len(index_and_score)):\n",
    "    if i >= n_best_size:\n",
    "      break\n",
    "    best_indexes.append(index_and_score[i][0])\n",
    "  return best_indexes\n",
    "\n",
    "\n",
    "def _compute_softmax(scores):\n",
    "  \"\"\"Compute softmax probability over raw logits.\"\"\"\n",
    "  if not scores:\n",
    "    return []\n",
    "\n",
    "  max_score = None\n",
    "  for score in scores:\n",
    "    if max_score is None or score > max_score:\n",
    "      max_score = score\n",
    "\n",
    "  exp_scores = []\n",
    "  total_sum = 0.0\n",
    "  for score in scores:\n",
    "    x = math.exp(score - max_score)\n",
    "    exp_scores.append(x)\n",
    "    total_sum += x\n",
    "\n",
    "  probs = []\n",
    "  for score in exp_scores:\n",
    "    probs.append(score / total_sum)\n",
    "  return probs\n",
    "\n",
    "\n",
    "class FeatureWriter(object):\n",
    "  \"\"\"Writes InputFeature to TF example file.\"\"\"\n",
    "\n",
    "  def __init__(self, filename, is_training):\n",
    "    self.filename = filename\n",
    "    self.is_training = is_training\n",
    "    self.num_features = 0\n",
    "    self._writer = tf.compat.v1.python_io.TFRecordWriter(filename)\n",
    "\n",
    "  def process_feature(self, feature):\n",
    "    \"\"\"Write a InputFeature to the TFRecordWriter as a tf.compat.v1.train.Example.\"\"\"\n",
    "    self.num_features += 1\n",
    "\n",
    "    def create_int_feature(values):\n",
    "      feature = tf.compat.v1.train.Feature(\n",
    "          int64_list=tf.compat.v1.train.Int64List(value=list(values)))\n",
    "      return feature\n",
    "\n",
    "    features = collections.OrderedDict()\n",
    "    features[\"unique_ids\"] = create_int_feature([feature.unique_id])\n",
    "    features[\"input_ids\"] = create_int_feature(feature.input_ids)\n",
    "    features[\"input_mask\"] = create_int_feature(feature.input_mask)\n",
    "    features[\"segment_ids\"] = create_int_feature(feature.segment_ids)\n",
    "\n",
    "    if self.is_training:\n",
    "      features[\"start_positions\"] = create_int_feature([feature.start_position])\n",
    "      features[\"end_positions\"] = create_int_feature([feature.end_position])\n",
    "      impossible = 1\n",
    "      if feature.answerable:\n",
    "        impossible = 0\n",
    "      features[\"answerable\"] = create_int_feature([impossible])\n",
    "\n",
    "    tf_example = tf.compat.v1.train.Example(features=tf.compat.v1.train.Features(feature=features))\n",
    "    self._writer.write(tf_example.SerializeToString())\n",
    "\n",
    "  def close(self):\n",
    "    self._writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f5f40cc5339d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"../bert_output\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "do_lower_case = True\n",
    "init_checkpoint = None\n",
    "\n",
    "null_score_diff_threshold = 0.0\n",
    "version_2_with_negative = False\n",
    "verbose_logging = False\n",
    "num_tpu_cores = 8\n",
    "master = None\n",
    "tpu_zone = None\n",
    "tpu_name = None\n",
    "use_tpu = False\n",
    "max_answer_length = 30\n",
    "n_best_size = 20\n",
    "iterations_per_loop = 1000\n",
    "save_checkpoints_steps = 1000\n",
    "warmup_proportion = 0.1\n",
    "predict_batch_size = 8\n",
    "\n",
    "max_query_length = 64\n",
    "doc_stride = 128\n",
    "max_seq_length = 384\n",
    "\n",
    "vocab_file=\"../chinese_L-12_H-768_A-12/vocab.txt\"\n",
    "bert_config_file=\"../chinese_L-12_H-768_A-12/bert_config.json\"\n",
    "do_train=True\n",
    "train_file=\"./data/train.json\"\n",
    "do_predict=True\n",
    "predict_file=\"./data/dev.json\"\n",
    "train_batch_size=12\n",
    "learning_rate=3e-5\n",
    "num_train_epochs=2.0\n",
    "output_dir=\"../bert_output\"\n",
    "\n",
    "tf.compat.v1.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_tokens = []\n",
    "char_to_word_offset = []\n",
    "\n",
    "paragraph_text = \"Hi, I am Wendy\"\n",
    "\n",
    "def is_whitespace(c):\n",
    "    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F or ord(c) == 0x80:\n",
    "      return True\n",
    "    return False\n",
    "prev_is_whitespace = True\n",
    "\n",
    "for c in paragraph_text:\n",
    "    if is_whitespace(c):\n",
    "      prev_is_whitespace = True\n",
    "    else:\n",
    "      if prev_is_whitespace:\n",
    "        doc_tokens.append(c)\n",
    "      else:\n",
    "        doc_tokens[-1] += c\n",
    "      prev_is_whitespace = False\n",
    "    char_to_word_offset.append(len(doc_tokens) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "print(char_to_word_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi,', 'I', 'am', 'Wendy']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_offset = 9\n",
    "answer_length = 5\n",
    "start_position = char_to_word_offset[answer_offset]\n",
    "end_position = char_to_word_offset[answer_offset + answer_length - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wendy']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_tokens[start_position:(end_position + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_position+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
