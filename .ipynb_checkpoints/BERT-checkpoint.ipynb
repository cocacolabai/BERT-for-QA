{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import *\n",
    "from tqdm.auto import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch = 10\n",
    "batch_size = 8\n",
    "lr = 1e-4\n",
    "weight_decay = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "bert_pretrain_name = 'bert-base-chinese'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_pretrain_name)\n",
    "model = BertForNextSentencePrediction.from_pretrained(bert_pretrain_name).to(device)\n",
    "optim = AdamW(model.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyDataset(Dataset):\n",
    "  def __init__(self, path: str, tokenizer: BertTokenizer) -> None:\n",
    "    self.tokenizer = tokenizer\n",
    "    self.data = []\n",
    "    with open(path) as f:\n",
    "      for article in json.load(f)['data']:\n",
    "        parapraphs = article['paragraphs']\n",
    "        for para in parapraphs:\n",
    "          context = para['context']\n",
    "          for qa in para['qas']:\n",
    "            qa_id = qa['id']\n",
    "            question = qa['question']\n",
    "            answerable = qa['answerable']\n",
    "            self.data.append((qa_id, context, question, answerable))\n",
    "  \n",
    "  def __len__(self) -> int:\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index: int):\n",
    "    qa_id, context, question, answerable = self.data[index]\n",
    "    return qa_id, context, question, int(answerable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EarlyDataset(\"./data/train.json\", tokenizer)\n",
    "valid_dataset = EarlyDataset(\"./data/dev.json\", tokenizer)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94136d8c02e443c8aa45e5e340ce768b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bf08e2d0c5045b180461156183a7469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4810.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0952b3cefdc34fd1b0ac813e8f511030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4810.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0231434e97e548199c3ed99c981a0197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4810.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bc493bcecda40c09388fd2d58d5d638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4810.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7667a422c5a948fd8d561a400c4dbe5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4810.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b3356740ef37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     loss, logits = model(next_sentence_label=answerable.to(device), \n\u001b[1;32m     10\u001b[0m                          **input_dict)\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in trange(max_epoch):\n",
    "  for batch in tqdm(train_loader):\n",
    "    ids, contexts, questions, answerable = batch\n",
    "    input_dict = tokenizer.batch_encode_plus(contexts, questions, \n",
    "                                             max_length=tokenizer.max_len, \n",
    "                                             pad_to_max_length=True,\n",
    "                                             return_tensors='pt')\n",
    "    input_dict = {k: v.to(device) for k, v in input_dict.items()}\n",
    "    loss, logits = model(next_sentence_label=answerable.to(device), \n",
    "                         **input_dict)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "    tqdm(train_loader).set_description(f\"train loss: {loss.item():.4f}\")\n",
    "  \n",
    "  for batch in tqdm(valid_loader):\n",
    "    ids, contexts, questions, answerable = batch\n",
    "    input_dict = tokenizer.batch_encode_plus(contexts, questions, \n",
    "                                             max_length=tokenizer.max_len, \n",
    "                                             pad_to_max_length=True,\n",
    "                                             return_tensors='pt')\n",
    "    input_dict = {k: v.to(device) for k, v in input_dict.items()}\n",
    "    loss, logits = model(next_sentence_label=answerable.to(device), \n",
    "                         **input_dict)\n",
    "    \n",
    "    tqdm(valid_loader).set_description(f\"val loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement optimization (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for optimization\u001b[0m\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'optimization'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-2a0b0df9ec71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0moptimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'optimization'"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel, BertForQuestionAnswering\n",
    "import collections\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import optimization\n",
    "import six\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "# setting device    \n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"using device\",device)\n",
    "\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-chinese\"  # 指定繁簡中文 BERT-BASE 預訓練模型\n",
    "model = BertForQuestionAnswering.from_pretrained(PRETRAINED_MODEL_NAME, output_attentions=True).to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "\n",
    "def main(_):\n",
    "  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n",
    "\n",
    "  bert_config = model.config\n",
    "\n",
    "  tf.io.gfile.makedirs(output_dir)\n",
    "  \n",
    "  tpu_cluster_resolver = None\n",
    "  if  use_tpu and  tpu_name:\n",
    "    tpu_cluster_resolver = tf.cluster_resolver.TPUClusterResolver(\n",
    "         tpu_name, zone= tpu_zone, project= gcp_project)\n",
    "\n",
    "  is_per_host = tf.compat.v1.estimator.tpu.InputPipelineConfig.PER_HOST_V2\n",
    "  run_config = tf.compat.v1.estimator.tpu.RunConfig(\n",
    "      cluster=tpu_cluster_resolver,\n",
    "      master= master,\n",
    "      model_dir= output_dir,\n",
    "      save_checkpoints_steps= save_checkpoints_steps,\n",
    "      tpu_config=tf.compat.v1.estimator.tpu.TPUConfig(\n",
    "          iterations_per_loop= iterations_per_loop,\n",
    "          num_shards= num_tpu_cores,\n",
    "          per_host_input_for_training=is_per_host))\n",
    "\n",
    "  train_examples = None\n",
    "  num_train_steps = None\n",
    "  num_warmup_steps = None\n",
    "  if  do_train:\n",
    "    train_examples = read_squad_examples(\n",
    "        input_file= train_file, is_training=True)\n",
    "    num_train_steps = int(\n",
    "        len(train_examples) /  train_batch_size *  num_train_epochs)\n",
    "    num_warmup_steps = int(num_train_steps *  warmup_proportion)\n",
    "\n",
    "    # Pre-shuffle the input to avoid having to make a very large shuffle\n",
    "    # buffer in in the `input_fn`.\n",
    "    rng = random.Random(12345)\n",
    "    rng.shuffle(train_examples)\n",
    "\n",
    "  model_fn = model_fn_builder(\n",
    "      init_checkpoint= init_checkpoint,\n",
    "      learning_rate= learning_rate,\n",
    "      num_train_steps=num_train_steps,\n",
    "      num_warmup_steps=num_warmup_steps,\n",
    "      use_tpu= use_tpu,\n",
    "      use_one_hot_embeddings= use_tpu)\n",
    "\n",
    "  # If TPU is not available, this will fall back to normal Estimator on CPU\n",
    "  # or GPU.\n",
    "  estimator = tf.compat.v1.estimator.tpu.TPUEstimator(\n",
    "      use_tpu= use_tpu,\n",
    "      model_fn=model_fn,\n",
    "      config=run_config,\n",
    "      train_batch_size= train_batch_size,\n",
    "      predict_batch_size= predict_batch_size)\n",
    "\n",
    "  if  do_train:\n",
    "    # We write to a temporary file to avoid storing very large constant tensors\n",
    "    # in memory.\n",
    "    train_writer = FeatureWriter(\n",
    "        filename=os.path.join( output_dir, \"train.tf_record\"),\n",
    "        is_training=True)\n",
    "    convert_examples_to_features(\n",
    "        examples=train_examples,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length= max_seq_length,\n",
    "        doc_stride= doc_stride,\n",
    "        max_query_length= max_query_length,\n",
    "        is_training=True,\n",
    "        output_fn=train_writer.process_feature)\n",
    "    train_writer.close()\n",
    "\n",
    "    tf.compat.v1.logging.info(\"***** Running training *****\")\n",
    "    tf.compat.v1.logging.info(\"  Num orig examples = %d\", len(train_examples))\n",
    "    tf.compat.v1.logging.info(\"  Num split examples = %d\", train_writer.num_features)\n",
    "    tf.compat.v1.logging.info(\"  Batch size = %d\",  train_batch_size)\n",
    "    tf.compat.v1.logging.info(\"  Num steps = %d\", num_train_steps)\n",
    "    del train_examples\n",
    "\n",
    "    train_input_fn = input_fn_builder(\n",
    "        input_file=train_writer.filename,\n",
    "        seq_length= max_seq_length,\n",
    "        is_training=True,\n",
    "        drop_remainder=True)\n",
    "    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "\n",
    "  if  do_predict:\n",
    "    eval_examples = read_squad_examples(\n",
    "        input_file= predict_file, is_training=False)\n",
    "\n",
    "    eval_writer = FeatureWriter(\n",
    "        filename=os.path.join( output_dir, \"eval.tf_record\"),\n",
    "        is_training=False)\n",
    "    eval_features = []\n",
    "\n",
    "    def append_feature(feature):\n",
    "      eval_features.append(feature)\n",
    "      eval_writer.process_feature(feature)\n",
    "\n",
    "    convert_examples_to_features(\n",
    "        examples=eval_examples,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length= max_seq_length,\n",
    "        doc_stride= doc_stride,\n",
    "        max_query_length= max_query_length,\n",
    "        is_training=False,\n",
    "        output_fn=append_feature)\n",
    "    eval_writer.close()\n",
    "\n",
    "    tf.compat.v1.logging.info(\"***** Running predictions *****\")\n",
    "    tf.compat.v1.logging.info(\"  Num orig examples = %d\", len(eval_examples))\n",
    "    tf.compat.v1.logging.info(\"  Num split examples = %d\", len(eval_features))\n",
    "    tf.compat.v1.logging.info(\"  Batch size = %d\",  predict_batch_size)\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    predict_input_fn = input_fn_builder(\n",
    "        input_file=eval_writer.filename,\n",
    "        seq_length= max_seq_length,\n",
    "        is_training=False,\n",
    "        drop_remainder=False)\n",
    "\n",
    "    # If running eval on the TPU, you will need to specify the number of\n",
    "    # steps.\n",
    "    all_results = []\n",
    "    for result in estimator.predict(\n",
    "        predict_input_fn, yield_single_examples=True):\n",
    "      if len(all_results) % 1000 == 0:\n",
    "        tf.compat.v1.logging.info(\"Processing example: %d\" % (len(all_results)))\n",
    "      unique_id = int(result[\"unique_ids\"])\n",
    "      start_logits = [float(x) for x in result[\"start_logits\"].flat]\n",
    "      end_logits = [float(x) for x in result[\"end_logits\"].flat]\n",
    "      all_results.append(\n",
    "          RawResult(\n",
    "              unique_id=unique_id,\n",
    "              start_logits=start_logits,\n",
    "              end_logits=end_logits))\n",
    "\n",
    "    output_prediction_file = os.path.join( output_dir, \"predictions.json\")\n",
    "    output_nbest_file = os.path.join( output_dir, \"nbest_predictions.json\")\n",
    "    output_null_log_odds_file = os.path.join( output_dir, \"null_odds.json\")\n",
    "\n",
    "    write_predictions(eval_examples, eval_features, all_results,\n",
    "                       n_best_size,  max_answer_length,\n",
    "                       do_lower_case, output_prediction_file,\n",
    "                      output_nbest_file, output_null_log_odds_file)\n",
    "    \n",
    "class SquadExample(object):\n",
    "  \"\"\"A single training/test example for simple sequence classification.\n",
    "\n",
    "     For examples without an answer, the start and end position are -1.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               qas_id,\n",
    "               question_text,\n",
    "               doc_tokens,\n",
    "               orig_answer_text=None,\n",
    "               start_position=None,\n",
    "               end_position=None,\n",
    "               answerable=True):\n",
    "    self.qas_id = qas_id\n",
    "    self.question_text = question_text\n",
    "    self.doc_tokens = doc_tokens\n",
    "    self.orig_answer_text = orig_answer_text\n",
    "    self.start_position = start_position\n",
    "    self.end_position = end_position\n",
    "    self.answerable = answerable\n",
    "\n",
    "  def __str__(self):\n",
    "    return self.__repr__()\n",
    "\n",
    "  def __repr__(self):\n",
    "    s = \"\"\n",
    "    s += \"qas_id: %s\" % (printable_text(self.qas_id))\n",
    "    s += \", question_text: %s\" % (\n",
    "        printable_text(self.question_text))\n",
    "    s += \", doc_tokens: [%s]\" % (\" \".join(self.doc_tokens))\n",
    "    if self.start_position:\n",
    "      s += \", start_position: %d\" % (self.start_position)\n",
    "    if self.start_position:\n",
    "      s += \", end_position: %d\" % (self.end_position)\n",
    "    if self.start_position:\n",
    "      s += \", answerable: %r\" % (self.answerable)\n",
    "    return s\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "  \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               unique_id,\n",
    "               example_index,\n",
    "               doc_span_index,\n",
    "               tokens,\n",
    "               token_to_orig_map,\n",
    "               token_is_max_context,\n",
    "               input_ids,\n",
    "               input_mask,\n",
    "               segment_ids,\n",
    "               start_position=None,\n",
    "               end_position=None,\n",
    "               answerable=None):\n",
    "    self.unique_id = unique_id\n",
    "    self.example_index = example_index\n",
    "    self.doc_span_index = doc_span_index\n",
    "    self.tokens = tokens\n",
    "    self.token_to_orig_map = token_to_orig_map\n",
    "    self.token_is_max_context = token_is_max_context\n",
    "    self.input_ids = input_ids\n",
    "    self.input_mask = input_mask\n",
    "    self.segment_ids = segment_ids\n",
    "    self.start_position = start_position\n",
    "    self.end_position = end_position\n",
    "    self.answerable = answerable\n",
    "\n",
    "def whitespace_tokenize(text):\n",
    "  \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n",
    "  text = text.strip()\n",
    "  if not text:\n",
    "    return []\n",
    "  tokens = text.split()\n",
    "  return tokens\n",
    "\n",
    "def printable_text(text):\n",
    "  \"\"\"Returns text encoded in a way suitable for print or `tf.logging`.\"\"\"\n",
    "\n",
    "  # These functions want `str` for both Python2 and Python3, but in one case\n",
    "  # it's a Unicode string and in the other it's a byte string.\n",
    "  if six.PY3:\n",
    "    if isinstance(text, str):\n",
    "      return text\n",
    "    elif isinstance(text, bytes):\n",
    "      return text.decode(\"utf-8\", \"ignore\")\n",
    "    else:\n",
    "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "  elif six.PY2:\n",
    "    if isinstance(text, str):\n",
    "      return text\n",
    "    elif isinstance(text, unicode):\n",
    "      return text.encode(\"utf-8\")\n",
    "    else:\n",
    "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "  else:\n",
    "    raise ValueError(\"Not running on Python2 or Python 3?\")\n",
    "    \n",
    "    \n",
    "def read_squad_examples(input_file, is_training):\n",
    "  \"\"\"Read a SQuAD json file into a list of SquadExample.\"\"\"\n",
    "  with tf.compat.v1.gfile.Open(input_file, \"r\") as reader:\n",
    "    input_data = json.load(reader)[\"data\"]\n",
    "\n",
    "  def is_whitespace(c):\n",
    "    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F or ord(c) == 0x80:\n",
    "      return True\n",
    "    return False\n",
    "\n",
    "  examples = []\n",
    "\n",
    "  for entry in input_data:\n",
    "    for paragraph in entry[\"paragraphs\"]:\n",
    "      paragraph_text = paragraph[\"context\"]\n",
    "      doc_tokens = []\n",
    "      char_to_word_offset = []\n",
    "      prev_is_whitespace = True\n",
    "      for c in paragraph_text:\n",
    "        if is_whitespace(c):\n",
    "          prev_is_whitespace = True\n",
    "        else:\n",
    "          if prev_is_whitespace:\n",
    "            doc_tokens.append(c)\n",
    "          else:\n",
    "            doc_tokens[-1] += c\n",
    "          prev_is_whitespace = False\n",
    "        char_to_word_offset.append(len(doc_tokens) - 1)\n",
    "\n",
    "      for qa in paragraph[\"qas\"]:\n",
    "        qas_id = qa[\"id\"]\n",
    "        question_text = qa[\"question\"]\n",
    "        start_position = None\n",
    "        end_position = None\n",
    "        orig_answer_text = None\n",
    "        answerable = True\n",
    "        if is_training:\n",
    "\n",
    "          if version_2_with_negative:\n",
    "            answerable = qa[\"answerable\"]\n",
    "          if (len(qa[\"answers\"]) != 1) and answerable:\n",
    "            raise ValueError(\n",
    "                \"For training, each question should have exactly 1 answer.\")\n",
    "          if answerable:\n",
    "            answer = qa[\"answers\"][0]\n",
    "            orig_answer_text = answer[\"text\"]\n",
    "            answer_offset = answer[\"answer_start\"]\n",
    "            answer_length = len(orig_answer_text)\n",
    "            start_position = char_to_word_offset[answer_offset]\n",
    "            end_position = char_to_word_offset[answer_offset + answer_length -\n",
    "                                               1]\n",
    "            # Only add answers where the text can be exactly recovered from the\n",
    "            # document. If this CAN'T happen it's likely due to weird Unicode\n",
    "            # stuff so we will just skip the example.\n",
    "            #\n",
    "            # Note that this means for training mode, every example is NOT\n",
    "            # guaranteed to be preserved.\n",
    "            actual_text = \" \".join(\n",
    "                doc_tokens[start_position:(end_position + 1)])\n",
    "            cleaned_answer_text = \" \".join(\n",
    "                whitespace_tokenize(orig_answer_text))\n",
    "            if actual_text.find(cleaned_answer_text) == -1:\n",
    "              tf.compat.v1.logging.warning(\"Could not find answer: '%s' vs. '%s'\",\n",
    "                                 actual_text, cleaned_answer_text)\n",
    "              continue\n",
    "          else:\n",
    "            start_position = -1\n",
    "            end_position = -1\n",
    "            orig_answer_text = \"\"\n",
    "\n",
    "        example = SquadExample(\n",
    "            qas_id=qas_id,\n",
    "            question_text=question_text,\n",
    "            doc_tokens=doc_tokens,\n",
    "            orig_answer_text=orig_answer_text,\n",
    "            start_position=start_position,\n",
    "            end_position=end_position,\n",
    "            answerable=answerable)\n",
    "        examples.append(example)\n",
    "  return examples\n",
    "\n",
    "\n",
    "def convert_examples_to_features(examples, tokenizer, max_seq_length,\n",
    "                                 doc_stride, max_query_length, is_training,\n",
    "                                 output_fn):\n",
    "  \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "  unique_id = 1000000000\n",
    "\n",
    "  for (example_index, example) in enumerate(examples):\n",
    "    query_tokens = tokenizer.tokenize(example.question_text)\n",
    "\n",
    "    if len(query_tokens) > max_query_length:\n",
    "      query_tokens = query_tokens[0:max_query_length]\n",
    "\n",
    "    tok_to_orig_index = []\n",
    "    orig_to_tok_index = []\n",
    "    all_doc_tokens = []\n",
    "    for (i, token) in enumerate(example.doc_tokens):\n",
    "      orig_to_tok_index.append(len(all_doc_tokens))\n",
    "      sub_tokens = tokenizer.tokenize(token)\n",
    "      for sub_token in sub_tokens:\n",
    "        tok_to_orig_index.append(i)\n",
    "        all_doc_tokens.append(sub_token)\n",
    "\n",
    "    tok_start_position = None\n",
    "    tok_end_position = None\n",
    "    if is_training and not example.answerable:\n",
    "      tok_start_position = -1\n",
    "      tok_end_position = -1\n",
    "    if is_training and example.answerable:\n",
    "      tok_start_position = orig_to_tok_index[example.start_position]\n",
    "      if example.end_position < len(example.doc_tokens) - 1:\n",
    "        tok_end_position = orig_to_tok_index[example.end_position + 1] - 1\n",
    "      else:\n",
    "        tok_end_position = len(all_doc_tokens) - 1\n",
    "      (tok_start_position, tok_end_position) = _improve_answer_span(\n",
    "          all_doc_tokens, tok_start_position, tok_end_position, tokenizer,\n",
    "          example.orig_answer_text)\n",
    "\n",
    "    # The -3 accounts for [CLS], [SEP] and [SEP]\n",
    "    max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n",
    "\n",
    "    # We can have documents that are longer than the maximum sequence length.\n",
    "    # To deal with this we do a sliding window approach, where we take chunks\n",
    "    # of the up to our max length with a stride of `doc_stride`.\n",
    "    _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "        \"DocSpan\", [\"start\", \"length\"])\n",
    "    doc_spans = []\n",
    "    start_offset = 0\n",
    "    while start_offset < len(all_doc_tokens):\n",
    "      length = len(all_doc_tokens) - start_offset\n",
    "      if length > max_tokens_for_doc:\n",
    "        length = max_tokens_for_doc\n",
    "      doc_spans.append(_DocSpan(start=start_offset, length=length))\n",
    "      if start_offset + length == len(all_doc_tokens):\n",
    "        break\n",
    "      start_offset += min(length, doc_stride)\n",
    "\n",
    "    for (doc_span_index, doc_span) in enumerate(doc_spans):\n",
    "      tokens = []\n",
    "      token_to_orig_map = {}\n",
    "      token_is_max_context = {}\n",
    "      segment_ids = []\n",
    "      tokens.append(\"[CLS]\")\n",
    "      segment_ids.append(0)\n",
    "      for token in query_tokens:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "      tokens.append(\"[SEP]\")\n",
    "      segment_ids.append(0)\n",
    "\n",
    "      for i in range(doc_span.length):\n",
    "        split_token_index = doc_span.start + i\n",
    "        token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n",
    "\n",
    "        is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n",
    "                                               split_token_index)\n",
    "        token_is_max_context[len(tokens)] = is_max_context\n",
    "        tokens.append(all_doc_tokens[split_token_index])\n",
    "        segment_ids.append(1)\n",
    "      tokens.append(\"[SEP]\")\n",
    "      segment_ids.append(1)\n",
    "\n",
    "      input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "      # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "      # tokens are attended to.\n",
    "      input_mask = [1] * len(input_ids)\n",
    "\n",
    "      # Zero-pad up to the sequence length.\n",
    "      while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "\n",
    "      assert len(input_ids) == max_seq_length\n",
    "      assert len(input_mask) == max_seq_length\n",
    "      assert len(segment_ids) == max_seq_length\n",
    "\n",
    "      start_position = None\n",
    "      end_position = None\n",
    "      if is_training and example.answerable:\n",
    "        # For training, if our document chunk does not contain an annotation\n",
    "        # we throw it out, since there is nothing to predict.\n",
    "        doc_start = doc_span.start\n",
    "        doc_end = doc_span.start + doc_span.length - 1\n",
    "        out_of_span = False\n",
    "        if not (tok_start_position >= doc_start and\n",
    "                tok_end_position <= doc_end):\n",
    "          out_of_span = True\n",
    "        if out_of_span:\n",
    "          start_position = 0\n",
    "          end_position = 0\n",
    "        else:\n",
    "          doc_offset = len(query_tokens) + 2\n",
    "          start_position = tok_start_position - doc_start + doc_offset\n",
    "          end_position = tok_end_position - doc_start + doc_offset\n",
    "\n",
    "      if is_training and not example.answerable:\n",
    "        start_position = 0\n",
    "        end_position = 0\n",
    "\n",
    "      if example_index < 20:\n",
    "        tf.compat.v1.logging.info(\"*** Example ***\")\n",
    "        tf.compat.v1.logging.info(\"unique_id: %s\" % (unique_id))\n",
    "        tf.compat.v1.logging.info(\"example_index: %s\" % (example_index))\n",
    "#         tf.compat.v1.logging.info(\"doc_span_index: %s\" % (doc_span_index))\n",
    "#         tf.compat.v1.logging.info(\"tokens: %s\" % \" \".join(\n",
    "#             [BertTokenizer.printable_text(x) for x in tokens]))\n",
    "#         tf.compat.v1.logging.info(\"token_to_orig_map: %s\" % \" \".join(\n",
    "#             [\"%d:%d\" % (x, y) for (x, y) in six.iteritems(token_to_orig_map)]))\n",
    "#         tf.compat.v1.logging.info(\"token_is_max_context: %s\" % \" \".join([\n",
    "#             \"%d:%s\" % (x, y) for (x, y) in six.iteritems(token_is_max_context)\n",
    "#         ]))\n",
    "#         tf.compat.v1.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "#         tf.compat.v1.logging.info(\n",
    "#             \"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "#         tf.compat.v1.logging.info(\n",
    "#             \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "        if is_training and not example.answerable:\n",
    "          tf.compat.v1.logging.info(\"impossible example\")\n",
    "        if is_training and example.answerable:\n",
    "          answer_text = \" \".join(tokens[start_position:(end_position + 1)])\n",
    "          tf.compat.v1.logging.info(\"start_position: %d\" % (start_position))\n",
    "          tf.compat.v1.logging.info(\"end_position: %d\" % (end_position))\n",
    "          tf.compat.v1.logging.info(\n",
    "              \"answer: %s\" % (printable_text(answer_text)))\n",
    "\n",
    "      feature = InputFeatures(\n",
    "          unique_id=unique_id,\n",
    "          example_index=example_index,\n",
    "          doc_span_index=doc_span_index,\n",
    "          tokens=tokens,\n",
    "          token_to_orig_map=token_to_orig_map,\n",
    "          token_is_max_context=token_is_max_context,\n",
    "          input_ids=input_ids,\n",
    "          input_mask=input_mask,\n",
    "          segment_ids=segment_ids,\n",
    "          start_position=start_position,\n",
    "          end_position=end_position,\n",
    "          answerable=example.answerable)\n",
    "\n",
    "      # Run callback\n",
    "      output_fn(feature)\n",
    "\n",
    "      unique_id += 1\n",
    "\n",
    "\n",
    "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n",
    "                         orig_answer_text):\n",
    "  \"\"\"Returns tokenized answer spans that better match the annotated answer.\"\"\"\n",
    "\n",
    "  # The SQuAD annotations are character based. We first project them to\n",
    "  # whitespace-tokenized words. But then after WordPiece BertTokenizer, we can\n",
    "  # often find a \"better match\". For example:\n",
    "  #\n",
    "  #   Question: What year was John Smith born?\n",
    "  #   Context: The leader was John Smith (1895-1943).\n",
    "  #   Answer: 1895\n",
    "  #\n",
    "  # The original whitespace-tokenized answer will be \"(1895-1943).\". However\n",
    "  # after BertTokenizer, our tokens will be \"( 1895 - 1943 ) .\". So we can match\n",
    "  # the exact answer, 1895.\n",
    "  #\n",
    "  # However, this is not always possible. Consider the following:\n",
    "  #\n",
    "  #   Question: What country is the top exporter of electornics?\n",
    "  #   Context: The Japanese electronics industry is the lagest in the world.\n",
    "  #   Answer: Japan\n",
    "  #\n",
    "  # In this case, the annotator chose \"Japan\" as a character sub-span of\n",
    "  # the word \"Japanese\". Since our WordPiece tokenizer does not split\n",
    "  # \"Japanese\", we just use \"Japanese\" as the annotation. This is fairly rare\n",
    "  # in SQuAD, but does happen.\n",
    "  tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n",
    "\n",
    "  for new_start in range(input_start, input_end + 1):\n",
    "    for new_end in range(input_end, new_start - 1, -1):\n",
    "      text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n",
    "      if text_span == tok_answer_text:\n",
    "        return (new_start, new_end)\n",
    "\n",
    "  return (input_start, input_end)\n",
    "\n",
    "\n",
    "def _check_is_max_context(doc_spans, cur_span_index, position):\n",
    "  \"\"\"Check if this is the 'max context' doc span for the token.\"\"\"\n",
    "\n",
    "  # Because of the sliding window approach taken to scoring documents, a single\n",
    "  # token can appear in multiple documents. E.g.\n",
    "  #  Doc: the man went to the store and bought a gallon of milk\n",
    "  #  Span A: the man went to the\n",
    "  #  Span B: to the store and bought\n",
    "  #  Span C: and bought a gallon of\n",
    "  #  ...\n",
    "  #\n",
    "  # Now the word 'bought' will have two scores from spans B and C. We only\n",
    "  # want to consider the score with \"maximum context\", which we define as\n",
    "  # the *minimum* of its left and right context (the *sum* of left and\n",
    "  # right context will always be the same, of course).\n",
    "  #\n",
    "  # In the example the maximum context for 'bought' would be span C since\n",
    "  # it has 1 left context and 3 right context, while span B has 4 left context\n",
    "  # and 0 right context.\n",
    "  best_score = None\n",
    "  best_span_index = None\n",
    "  for (span_index, doc_span) in enumerate(doc_spans):\n",
    "    end = doc_span.start + doc_span.length - 1\n",
    "    if position < doc_span.start:\n",
    "      continue\n",
    "    if position > end:\n",
    "      continue\n",
    "    num_left_context = position - doc_span.start\n",
    "    num_right_context = end - position\n",
    "    score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n",
    "    if best_score is None or score > best_score:\n",
    "      best_score = score\n",
    "      best_span_index = span_index\n",
    "\n",
    "  return cur_span_index == best_span_index\n",
    "\n",
    "def get_shape_list(tensor, expected_rank=None, name=None):\n",
    "    if name is None:\n",
    "        name = tensor.name\n",
    "\n",
    "    if expected_rank is not None:\n",
    "        assert_rank(tensor, expected_rank, name)\n",
    "\n",
    "    shape = tensor.shape.as_list()\n",
    "\n",
    "    non_static_indexes = []\n",
    "    for (index, dim) in enumerate(shape):\n",
    "        if dim is None:\n",
    "            non_static_indexes.append(index)\n",
    "\n",
    "    if not non_static_indexes:\n",
    "        return shape\n",
    "\n",
    "    dyn_shape = tf.shape(tensor)\n",
    "    for index in non_static_indexes:\n",
    "        shape[index] = dyn_shape[index]\n",
    "    return shape\n",
    "\n",
    "def create_model(is_training, input_ids, segment_ids,\n",
    "                 use_one_hot_embeddings):\n",
    "  \"\"\"Creates a classification model.\"\"\"\n",
    "\n",
    "  \n",
    "\n",
    "  final_hidden = model.get_sequence_output()\n",
    "\n",
    "  final_hidden_shape = get_shape_list(final_hidden, expected_rank=3)\n",
    "  batch_size = final_hidden_shape[0]\n",
    "  seq_length = final_hidden_shape[1]\n",
    "  hidden_size = final_hidden_shape[2]\n",
    "\n",
    "  output_weights = tf.compat.v1.get_variable(\n",
    "      \"cls/squad/output_weights\", [2, hidden_size],\n",
    "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "  output_bias = tf.compat.v1.get_variable(\n",
    "      \"cls/squad/output_bias\", [2], initializer=tf.zeros_initializer())\n",
    "\n",
    "  final_hidden_matrix = tf.reshape(final_hidden,\n",
    "                                   [batch_size * seq_length, hidden_size])\n",
    "  logits = tf.matmul(final_hidden_matrix, output_weights, transpose_b=True)\n",
    "  logits = tf.nn.bias_add(logits, output_bias)\n",
    "\n",
    "  logits = tf.reshape(logits, [batch_size, seq_length, 2])\n",
    "  logits = tf.transpose(logits, [2, 0, 1])\n",
    "\n",
    "  unstacked_logits = tf.unstack(logits, axis=0)\n",
    "\n",
    "  (start_logits, end_logits) = (unstacked_logits[0], unstacked_logits[1])\n",
    "\n",
    "  return (start_logits, end_logits)\n",
    "\n",
    "\n",
    "\n",
    "def model_fn_builder(init_checkpoint, learning_rate,\n",
    "                     num_train_steps, num_warmup_steps, use_tpu,\n",
    "                     use_one_hot_embeddings):\n",
    "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "  def get_assignment_map_from_checkpoint(tvars, init_checkpoint):\n",
    "    \"\"\"Compute the union of the current variables and checkpoint variables.\"\"\"\n",
    "    assignment_map = {}\n",
    "    initialized_variable_names = {}\n",
    "\n",
    "    name_to_variable = collections.OrderedDict()\n",
    "    for var in tvars:\n",
    "        name = var.name\n",
    "        m = re.match(\"^(.*):\\\\d+$\", name)\n",
    "        if m is not None:\n",
    "            name = m.group(1)\n",
    "        name_to_variable[name] = var\n",
    "\n",
    "    init_vars = tf.train.list_variables(init_checkpoint)\n",
    "\n",
    "    assignment_map = collections.OrderedDict()\n",
    "    for x in init_vars:\n",
    "        (name, var) = (x[0], x[1])\n",
    "        if name not in name_to_variable:\n",
    "            continue\n",
    "        assignment_map[name] = name_to_variable[name]\n",
    "        initialized_variable_names[name] = 1\n",
    "        initialized_variable_names[name + \":0\"] = 1\n",
    "\n",
    "    return (assignment_map, initialized_variable_names)\n",
    "\n",
    "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "\n",
    "    tf.compat.v1.logging.info(\"*** Features ***\")\n",
    "    for name in sorted(features.keys()):\n",
    "      tf.compat.v1.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
    "\n",
    "    unique_ids = features[\"unique_ids\"]\n",
    "    input_ids = features[\"input_ids\"]\n",
    "    input_mask = features[\"input_mask\"]\n",
    "    segment_ids = features[\"segment_ids\"]\n",
    "\n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    (start_logits, end_logits) = create_model(\n",
    "        is_training=is_training,\n",
    "        input_ids=input_ids,\n",
    "        segment_ids=segment_ids,\n",
    "        use_one_hot_embeddings=use_one_hot_embeddings)\n",
    "\n",
    "    tvars = tf.compat.v1.trainable_variables()\n",
    "\n",
    "    initialized_variable_names = {}\n",
    "    scaffold_fn = None\n",
    "    if init_checkpoint:\n",
    "      (assignment_map, initialized_variable_names\n",
    "      ) = get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
    "      if use_tpu:\n",
    "\n",
    "        def tpu_scaffold():\n",
    "          tf.compat.v1.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "          return tf.compat.v1.train.Scaffold()\n",
    "\n",
    "        scaffold_fn = tpu_scaffold\n",
    "      else:\n",
    "        tf.compat.v1.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "\n",
    "    tf.compat.v1.logging.info(\"**** Trainable Variables ****\")\n",
    "    for var in tvars:\n",
    "      init_string = \"\"\n",
    "      if var.name in initialized_variable_names:\n",
    "        init_string = \", *INIT_FROM_CKPT*\"\n",
    "      tf.compat.v1.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n",
    "                      init_string)\n",
    "\n",
    "    output_spec = None\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "      seq_length = get_shape_list(input_ids)[1]\n",
    "\n",
    "      def compute_loss(logits, positions):\n",
    "        one_hot_positions = tf.one_hot(\n",
    "            positions, depth=seq_length, dtype=tf.float32)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "        loss = -tf.reduce_mean(\n",
    "            tf.reduce_sum(one_hot_positions * log_probs, axis=-1))\n",
    "        return loss\n",
    "\n",
    "      start_positions = features[\"start_positions\"]\n",
    "      end_positions = features[\"end_positions\"]\n",
    "\n",
    "      start_loss = compute_loss(start_logits, start_positions)\n",
    "      end_loss = compute_loss(end_logits, end_positions)\n",
    "\n",
    "      total_loss = (start_loss + end_loss) / 2.0\n",
    "\n",
    "      train_op = optimization.create_optimizer(\n",
    "          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n",
    "\n",
    "      output_spec = tf.compat.v1.estimator.tpu.TPUEstimatorSpec(\n",
    "          mode=mode,\n",
    "          loss=total_loss,\n",
    "          train_op=train_op,\n",
    "          scaffold_fn=scaffold_fn)\n",
    "    elif mode == tf.estimator.ModeKeys.PREDICT:\n",
    "      predictions = {\n",
    "          \"unique_ids\": unique_ids,\n",
    "          \"start_logits\": start_logits,\n",
    "          \"end_logits\": end_logits,\n",
    "      }\n",
    "      output_spec = tf.compat.v1.estimator.tpu.TPUEstimatorSpec(\n",
    "          mode=mode, predictions=predictions, scaffold_fn=scaffold_fn)\n",
    "    else:\n",
    "      raise ValueError(\n",
    "          \"Only TRAIN and PREDICT modes are supported: %s\" % (mode))\n",
    "\n",
    "    return output_spec\n",
    "\n",
    "  return model_fn\n",
    "\n",
    "\n",
    "def input_fn_builder(input_file, seq_length, is_training, drop_remainder):\n",
    "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
    "\n",
    "  name_to_features = {\n",
    "      \"unique_ids\": tf.io.FixedLenFeature([], tf.int64),\n",
    "      \"input_ids\": tf.io.FixedLenFeature([seq_length], tf.int64),\n",
    "      \"input_mask\": tf.io.FixedLenFeature([seq_length], tf.int64),\n",
    "      \"segment_ids\": tf.io.FixedLenFeature([seq_length], tf.int64),\n",
    "  }\n",
    "\n",
    "  if is_training:\n",
    "    name_to_features[\"start_positions\"] = tf.io.FixedLenFeature([], tf.int64)\n",
    "    name_to_features[\"end_positions\"] = tf.io.FixedLenFeature([], tf.int64)\n",
    "\n",
    "  def _decode_record(record, name_to_features):\n",
    "    \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "    example = tf.io.parse_single_example(record, name_to_features)\n",
    "\n",
    "    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
    "    # So cast all int64 to int32.\n",
    "    for name in list(example.keys()):\n",
    "      t = example[name]\n",
    "      if t.dtype == tf.int64:\n",
    "        t = tf.compat.v1.to_int32(t)\n",
    "      example[name] = t\n",
    "\n",
    "    return example\n",
    "\n",
    "  def input_fn(params):\n",
    "    \"\"\"The actual input function.\"\"\"\n",
    "    batch_size = params[\"batch_size\"]\n",
    "\n",
    "    # For training, we want a lot of parallel reading and shuffling.\n",
    "    # For eval, we want no shuffling and parallel reading doesn't matter.\n",
    "    d = tf.data.TFRecordDataset(input_file)\n",
    "    if is_training:\n",
    "      d = d.repeat()\n",
    "      d = d.shuffle(buffer_size=100)\n",
    "\n",
    "    d = d.apply(\n",
    "        tf.data.experimental.map_and_batch(\n",
    "            lambda record: _decode_record(record, name_to_features),\n",
    "            batch_size=batch_size,\n",
    "            drop_remainder=drop_remainder))\n",
    "\n",
    "    return d\n",
    "\n",
    "  return input_fn\n",
    "\n",
    "\n",
    "RawResult = collections.namedtuple(\"RawResult\",\n",
    "                                   [\"unique_id\", \"start_logits\", \"end_logits\"])\n",
    "\n",
    "\n",
    "def write_predictions(all_examples, all_features, all_results, n_best_size,\n",
    "                      max_answer_length, do_lower_case, output_prediction_file,\n",
    "                      output_nbest_file, output_null_log_odds_file):\n",
    "  \"\"\"Write final predictions to the json file and log-odds of null if needed.\"\"\"\n",
    "  tf.compat.v1.logging.info(\"Writing predictions to: %s\" % (output_prediction_file))\n",
    "  tf.compat.v1.logging.info(\"Writing nbest to: %s\" % (output_nbest_file))\n",
    "\n",
    "  example_index_to_features = collections.defaultdict(list)\n",
    "  for feature in all_features:\n",
    "    example_index_to_features[feature.example_index].append(feature)\n",
    "\n",
    "  unique_id_to_result = {}\n",
    "  for result in all_results:\n",
    "    unique_id_to_result[result.unique_id] = result\n",
    "\n",
    "  _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "      \"PrelimPrediction\",\n",
    "      [\"feature_index\", \"start_index\", \"end_index\", \"start_logit\", \"end_logit\"])\n",
    "\n",
    "  all_predictions = collections.OrderedDict()\n",
    "  all_nbest_json = collections.OrderedDict()\n",
    "  scores_diff_json = collections.OrderedDict()\n",
    "\n",
    "  for (example_index, example) in enumerate(all_examples):\n",
    "    features = example_index_to_features[example_index]\n",
    "\n",
    "    prelim_predictions = []\n",
    "    # keep track of the minimum score of null start+end of position 0\n",
    "    score_null = 1000000  # large and positive\n",
    "    min_null_feature_index = 0  # the paragraph slice with min mull score\n",
    "    null_start_logit = 0  # the start logit at the slice with min null score\n",
    "    null_end_logit = 0  # the end logit at the slice with min null score\n",
    "    for (feature_index, feature) in enumerate(features):\n",
    "      result = unique_id_to_result[feature.unique_id]\n",
    "      start_indexes = _get_best_indexes(result.start_logits, n_best_size)\n",
    "      end_indexes = _get_best_indexes(result.end_logits, n_best_size)\n",
    "      # if we could have irrelevant answers, get the min score of irrelevant\n",
    "      if  version_2_with_negative:\n",
    "        feature_null_score = result.start_logits[0] + result.end_logits[0]\n",
    "        if feature_null_score < score_null:\n",
    "          score_null = feature_null_score\n",
    "          min_null_feature_index = feature_index\n",
    "          null_start_logit = result.start_logits[0]\n",
    "          null_end_logit = result.end_logits[0]\n",
    "      for start_index in start_indexes:\n",
    "        for end_index in end_indexes:\n",
    "          # We could hypothetically create invalid predictions, e.g., predict\n",
    "          # that the start of the span is in the question. We throw out all\n",
    "          # invalid predictions.\n",
    "          if start_index >= len(feature.tokens):\n",
    "            continue\n",
    "          if end_index >= len(feature.tokens):\n",
    "            continue\n",
    "          if start_index not in feature.token_to_orig_map:\n",
    "            continue\n",
    "          if end_index not in feature.token_to_orig_map:\n",
    "            continue\n",
    "          if not feature.token_is_max_context.get(start_index, False):\n",
    "            continue\n",
    "          if end_index < start_index:\n",
    "            continue\n",
    "          length = end_index - start_index + 1\n",
    "          if length > max_answer_length:\n",
    "            continue\n",
    "          prelim_predictions.append(\n",
    "              _PrelimPrediction(\n",
    "                  feature_index=feature_index,\n",
    "                  start_index=start_index,\n",
    "                  end_index=end_index,\n",
    "                  start_logit=result.start_logits[start_index],\n",
    "                  end_logit=result.end_logits[end_index]))\n",
    "\n",
    "    if  version_2_with_negative:\n",
    "      prelim_predictions.append(\n",
    "          _PrelimPrediction(\n",
    "              feature_index=min_null_feature_index,\n",
    "              start_index=0,\n",
    "              end_index=0,\n",
    "              start_logit=null_start_logit,\n",
    "              end_logit=null_end_logit))\n",
    "    prelim_predictions = sorted(\n",
    "        prelim_predictions,\n",
    "        . (x.start_logit + x.end_logit),\n",
    "        reverse=True)\n",
    "\n",
    "    _NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "        \"NbestPrediction\", [\"text\", \"start_logit\", \"end_logit\"])\n",
    "\n",
    "    seen_predictions = {}\n",
    "    nbest = []\n",
    "    for pred in prelim_predictions:\n",
    "      if len(nbest) >= n_best_size:\n",
    "        break\n",
    "      feature = features[pred.feature_index]\n",
    "      if pred.start_index > 0:  # this is a non-null prediction\n",
    "        tok_tokens = feature.tokens[pred.start_index:(pred.end_index + 1)]\n",
    "        orig_doc_start = feature.token_to_orig_map[pred.start_index]\n",
    "        orig_doc_end = feature.token_to_orig_map[pred.end_index]\n",
    "        orig_tokens = example.doc_tokens[orig_doc_start:(orig_doc_end + 1)]\n",
    "        tok_text = \" \".join(tok_tokens)\n",
    "\n",
    "        # De-tokenize WordPieces that have been split off.\n",
    "        tok_text = tok_text.replace(\" ##\", \"\")\n",
    "        tok_text = tok_text.replace(\"##\", \"\")\n",
    "\n",
    "        # Clean whitespace\n",
    "        tok_text = tok_text.strip()\n",
    "        tok_text = \" \".join(tok_text.split())\n",
    "        orig_text = \" \".join(orig_tokens)\n",
    "\n",
    "        final_text = get_final_text(tok_text, orig_text, do_lower_case)\n",
    "        if final_text in seen_predictions:\n",
    "          continue\n",
    "\n",
    "        seen_predictions[final_text] = True\n",
    "      else:\n",
    "        final_text = \"\"\n",
    "        seen_predictions[final_text] = True\n",
    "\n",
    "      nbest.append(\n",
    "          _NbestPrediction(\n",
    "              text=final_text,\n",
    "              start_logit=pred.start_logit,\n",
    "              end_logit=pred.end_logit))\n",
    "\n",
    "    # if we didn't inlude the empty option in the n-best, inlcude it\n",
    "    if  version_2_with_negative:\n",
    "      if \"\" not in seen_predictions:\n",
    "        nbest.append(\n",
    "            _NbestPrediction(\n",
    "                text=\"\", start_logit=null_start_logit,\n",
    "                end_logit=null_end_logit))\n",
    "    # In very rare edge cases we could have no valid predictions. So we\n",
    "    # just create a nonce prediction in this case to avoid failure.\n",
    "    if not nbest:\n",
    "      nbest.append(\n",
    "          _NbestPrediction(text=\"empty\", start_logit=0.0, end_logit=0.0))\n",
    "\n",
    "    assert len(nbest) >= 1\n",
    "\n",
    "    total_scores = []\n",
    "    best_non_null_entry = None\n",
    "    for entry in nbest:\n",
    "      total_scores.append(entry.start_logit + entry.end_logit)\n",
    "      if not best_non_null_entry:\n",
    "        if entry.text:\n",
    "          best_non_null_entry = entry\n",
    "\n",
    "    probs = _compute_softmax(total_scores)\n",
    "\n",
    "    nbest_json = []\n",
    "    for (i, entry) in enumerate(nbest):\n",
    "      output = collections.OrderedDict()\n",
    "      output[\"text\"] = entry.text\n",
    "      output[\"probability\"] = probs[i]\n",
    "      output[\"start_logit\"] = entry.start_logit\n",
    "      output[\"end_logit\"] = entry.end_logit\n",
    "      nbest_json.append(output)\n",
    "\n",
    "    assert len(nbest_json) >= 1\n",
    "\n",
    "    if not  version_2_with_negative:\n",
    "      all_predictions[example.qas_id] = nbest_json[0][\"text\"]\n",
    "    else:\n",
    "      # predict \"\" iff the null score - the score of best non-null > threshold\n",
    "      score_diff = score_null - best_non_null_entry.start_logit - (\n",
    "          best_non_null_entry.end_logit)\n",
    "      scores_diff_json[example.qas_id] = score_diff\n",
    "      if score_diff >  null_score_diff_threshold:\n",
    "        all_predictions[example.qas_id] = \"\"\n",
    "      else:\n",
    "        all_predictions[example.qas_id] = best_non_null_entry.text\n",
    "\n",
    "    all_nbest_json[example.qas_id] = nbest_json\n",
    "\n",
    "  with tf.io.gfile.GFile(output_prediction_file, \"w\") as writer:\n",
    "    writer.write(json.dumps(all_predictions, indent=4) + \"\\n\")\n",
    "\n",
    "  with tf.io.gfile.GFile(output_nbest_file, \"w\") as writer:\n",
    "    writer.write(json.dumps(all_nbest_json, indent=4) + \"\\n\")\n",
    "\n",
    "  if  version_2_with_negative:\n",
    "    with tf.io.gfile.GFile(output_null_log_odds_file, \"w\") as writer:\n",
    "      writer.write(json.dumps(scores_diff_json, indent=4) + \"\\n\")\n",
    "\n",
    "\n",
    "def get_final_text(pred_text, orig_text, do_lower_case):\n",
    "  \"\"\"Project the tokenized prediction back to the original text.\"\"\"\n",
    "\n",
    "  # When we created the data, we kept track of the alignment between original\n",
    "  # (whitespace tokenized) tokens and our WordPiece tokenized tokens. So\n",
    "  # now `orig_text` contains the span of our original text corresponding to the\n",
    "  # span that we predicted.\n",
    "  #\n",
    "  # However, `orig_text` may contain extra characters that we don't want in\n",
    "  # our prediction.\n",
    "  #\n",
    "  # For example, let's say:\n",
    "  #   pred_text = steve smith\n",
    "  #   orig_text = Steve Smith's\n",
    "  #\n",
    "  # We don't want to return `orig_text` because it contains the extra \"'s\".\n",
    "  #\n",
    "  # We don't want to return `pred_text` because it's already been normalized\n",
    "  # (the SQuAD eval script also does punctuation stripping/lower casing but\n",
    "  # our tokenizer does additional normalization like stripping accent\n",
    "  # characters).\n",
    "  #\n",
    "  # What we really want to return is \"Steve Smith\".\n",
    "  #\n",
    "  # Therefore, we have to apply a semi-complicated alignment heruistic between\n",
    "  # `pred_text` and `orig_text` to get a character-to-charcter alignment. This\n",
    "  # can fail in certain cases in which case we just return `orig_text`.\n",
    "\n",
    "  def _strip_spaces(text):\n",
    "    ns_chars = []\n",
    "    ns_to_s_map = collections.OrderedDict()\n",
    "    for (i, c) in enumerate(text):\n",
    "      if c == \" \":\n",
    "        continue\n",
    "      ns_to_s_map[len(ns_chars)] = i\n",
    "      ns_chars.append(c)\n",
    "    ns_text = \"\".join(ns_chars)\n",
    "    return (ns_text, ns_to_s_map)\n",
    "\n",
    "  # We first tokenize `orig_text`, strip whitespace from the result\n",
    "  # and `pred_text`, and check if they are the same length. If they are\n",
    "  # NOT the same length, the heuristic has failed. If they are the same\n",
    "  # length, we assume the characters are one-to-one aligned.\n",
    "\n",
    "  tok_text = \" \".join(tokenizer.tokenize(orig_text))\n",
    "\n",
    "  start_position = tok_text.find(pred_text)\n",
    "  if start_position == -1:\n",
    "    if  verbose_logging:\n",
    "      tf.compat.v1.logging.info(\n",
    "          \"Unable to find text: '%s' in '%s'\" % (pred_text, orig_text))\n",
    "    return orig_text\n",
    "  end_position = start_position + len(pred_text) - 1\n",
    "\n",
    "  (orig_ns_text, orig_ns_to_s_map) = _strip_spaces(orig_text)\n",
    "  (tok_ns_text, tok_ns_to_s_map) = _strip_spaces(tok_text)\n",
    "\n",
    "  if len(orig_ns_text) != len(tok_ns_text):\n",
    "    if  verbose_logging:\n",
    "      tf.compat.v1.logging.info(\"Length not equal after stripping spaces: '%s' vs '%s'\",\n",
    "                      orig_ns_text, tok_ns_text)\n",
    "    return orig_text\n",
    "\n",
    "  # We then project the characters in `pred_text` back to `orig_text` using\n",
    "  # the character-to-character alignment.\n",
    "  tok_s_to_ns_map = {}\n",
    "  for (i, tok_index) in six.iteritems(tok_ns_to_s_map):\n",
    "    tok_s_to_ns_map[tok_index] = i\n",
    "\n",
    "  orig_start_position = None\n",
    "  if start_position in tok_s_to_ns_map:\n",
    "    ns_start_position = tok_s_to_ns_map[start_position]\n",
    "    if ns_start_position in orig_ns_to_s_map:\n",
    "      orig_start_position = orig_ns_to_s_map[ns_start_position]\n",
    "\n",
    "  if orig_start_position is None:\n",
    "    if  verbose_logging:\n",
    "      tf.compat.v1.logging.info(\"Couldn't map start position\")\n",
    "    return orig_text\n",
    "\n",
    "  orig_end_position = None\n",
    "  if end_position in tok_s_to_ns_map:\n",
    "    ns_end_position = tok_s_to_ns_map[end_position]\n",
    "    if ns_end_position in orig_ns_to_s_map:\n",
    "      orig_end_position = orig_ns_to_s_map[ns_end_position]\n",
    "\n",
    "  if orig_end_position is None:\n",
    "    if  verbose_logging:\n",
    "      tf.compat.v1.logging.info(\"Couldn't map end position\")\n",
    "    return orig_text\n",
    "\n",
    "  output_text = orig_text[orig_start_position:(orig_end_position + 1)]\n",
    "  return output_text\n",
    "\n",
    "\n",
    "def _get_best_indexes(logits, n_best_size):\n",
    "  \"\"\"Get the n-best logits from a list.\"\"\"\n",
    "  index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "  best_indexes = []\n",
    "  for i in range(len(index_and_score)):\n",
    "    if i >= n_best_size:\n",
    "      break\n",
    "    best_indexes.append(index_and_score[i][0])\n",
    "  return best_indexes\n",
    "\n",
    "\n",
    "def _compute_softmax(scores):\n",
    "  \"\"\"Compute softmax probability over raw logits.\"\"\"\n",
    "  if not scores:\n",
    "    return []\n",
    "\n",
    "  max_score = None\n",
    "  for score in scores:\n",
    "    if max_score is None or score > max_score:\n",
    "      max_score = score\n",
    "\n",
    "  exp_scores = []\n",
    "  total_sum = 0.0\n",
    "  for score in scores:\n",
    "    x = math.exp(score - max_score)\n",
    "    exp_scores.append(x)\n",
    "    total_sum += x\n",
    "\n",
    "  probs = []\n",
    "  for score in exp_scores:\n",
    "    probs.append(score / total_sum)\n",
    "  return probs\n",
    "\n",
    "\n",
    "class FeatureWriter(object):\n",
    "  \"\"\"Writes InputFeature to TF example file.\"\"\"\n",
    "\n",
    "  def __init__(self, filename, is_training):\n",
    "    self.filename = filename\n",
    "    self.is_training = is_training\n",
    "    self.num_features = 0\n",
    "    self._writer = tf.compat.v1.python_io.TFRecordWriter(filename)\n",
    "\n",
    "  def process_feature(self, feature):\n",
    "    \"\"\"Write a InputFeature to the TFRecordWriter as a tf.compat.v1.train.Example.\"\"\"\n",
    "    self.num_features += 1\n",
    "\n",
    "    def create_int_feature(values):\n",
    "      feature = tf.compat.v1.train.Feature(\n",
    "          int64_list=tf.compat.v1.train.Int64List(value=list(values)))\n",
    "      return feature\n",
    "\n",
    "    features = collections.OrderedDict()\n",
    "    features[\"unique_ids\"] = create_int_feature([feature.unique_id])\n",
    "    features[\"input_ids\"] = create_int_feature(feature.input_ids)\n",
    "    features[\"input_mask\"] = create_int_feature(feature.input_mask)\n",
    "    features[\"segment_ids\"] = create_int_feature(feature.segment_ids)\n",
    "\n",
    "    if self.is_training:\n",
    "      features[\"start_positions\"] = create_int_feature([feature.start_position])\n",
    "      features[\"end_positions\"] = create_int_feature([feature.end_position])\n",
    "      impossible = 1\n",
    "      if feature.answerable:\n",
    "        impossible = 0\n",
    "      features[\"answerable\"] = create_int_feature([impossible])\n",
    "\n",
    "    tf_example = tf.compat.v1.train.Example(features=tf.compat.v1.train.Features(feature=features))\n",
    "    self._writer.write(tf_example.SerializeToString())\n",
    "\n",
    "  def close(self):\n",
    "    self._writer.close()\n",
    "\n",
    "do_lower_case = True\n",
    "init_checkpoint = None\n",
    "\n",
    "null_score_diff_threshold = 0.0\n",
    "version_2_with_negative = False\n",
    "verbose_logging = False\n",
    "num_tpu_cores = 8\n",
    "master = None\n",
    "tpu_zone = None\n",
    "tpu_name = None\n",
    "use_tpu = False\n",
    "max_answer_length = 30\n",
    "n_best_size = 20\n",
    "iterations_per_loop = 1000\n",
    "save_checkpoints_steps = 1000\n",
    "warmup_proportion = 0.1\n",
    "predict_batch_size = 8\n",
    "\n",
    "max_query_length = 64\n",
    "doc_stride = 128\n",
    "max_seq_length = 384\n",
    "\n",
    "vocab_file=\"../chinese_L-12_H-768_A-12/vocab.txt\"\n",
    "bert_config_file=\"../chinese_L-12_H-768_A-12/bert_config.json\"\n",
    "do_train=True\n",
    "train_file=\"./data/train-small.json\" ###\n",
    "do_predict=True\n",
    "predict_file=\"./data/dev-small.json\" ###\n",
    "train_batch_size=12\n",
    "learning_rate=3e-5\n",
    "num_train_epochs=1.0 ###\n",
    "output_dir=\"../bert_alone_output\"\n",
    "\n",
    "tf.compat.v1.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a=torch.tensor([[1,2],[3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "for i in a[0]>0:\n",
    "    if i:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids= [\"a\",\"b\"]\n",
    "all_predictions = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,id in zip(a[0]>0, ids):\n",
    "    if i:\n",
    "        all_predictions[id] = \"\"\n",
    "    else:\n",
    "        all_predictions[id] = \"have answer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': '', 'b': ''}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "output_file = Path(\"ha.json\")\n",
    "output_file.write_text(json.dumps(all_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame()\n",
    "df['id']=[1,2,3]\n",
    "df['name']=['we','wef','asd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在歐洲，梵語的學術研究，由德國學者陸特和漢斯雷頓開創。後來威廉·瓊斯發現印歐語系，也要歸功於對梵語的研究。此外，梵語研究，也對西方文字學及歷史語言學的發展，貢獻不少。1786年2月2日，亞洲協會在加爾各答舉行。會中，威廉·瓊斯發表了下面這段著名的言論：「梵語儘管非常古老，構造卻精妙絕倫：比希臘語還完美，比拉丁語還豐富，精緻之處同時勝過此兩者，但在動詞詞根和語法形式上，又跟此兩者無比相似，不可能是巧合的結果。這三種語言太相似了，使任何同時稽考三者的語文學家都不得不相信三者同出一源，出自一種可能已經消逝的語言。基於相似的原因，儘管缺少同樣有力的證據，我們可以推想哥德語和凱爾特語，雖然混入了迥然不同的語彙，也與梵語有著相同的起源；而古波斯語可能也是這一語系的子裔。」\n",
      "梵語在社交中口頭使用，並且在早期古典梵語文獻的發展中維持口頭傳統。在印度，書寫形式是當梵語發展成俗語之後才出現的；在書寫梵語的時候，書寫系統的選擇受抄寫者所處地域的影響。同樣的，所有南亞的主要書寫系統事實上都用於梵語文稿的抄寫。自19世紀晚期，天城文被定為梵語的標準書寫系統，十分可能的原因是歐洲人有用這種文字印刷梵語文本的習慣。最早的已知梵語碑刻可確定為公元前一世紀。它們採用了最初用於俗語而非梵語的婆羅米文。第一個書寫梵語的證據，出現在晚於它的俗語的書寫證據之後的幾個世紀，這被描述為一種悖論。在梵語被書寫下來的時候，它首先用於行政、文學或科學類的文本。宗教文本口頭傳承，在相當晚的時候才「不情願」地被書寫下來。\n",
      "梵語中，厚重、美觀的蘭札體流行於西藏和尼泊爾，並且隨著中國清朝統治階層對藏傳佛教的信仰，流行於中國漢地。西藏同時還使用一種叫做瓦德體的字體來書寫梵文，其實它只是蘭札體的一種無頭化變體。藏文、蒙文、托忒蒙文、滿文都有用各自的文字轉寫梵文的體系。1991年印度人口普查報告了49,736名流利的梵語使用者。自從1990年代，對復興口頭使用梵語的努力已經加強了。很多組織如「山姆卡地巴蒂」引領著說梵語講習班來普及這門語言。印度的「中學教育中央委員會」已經使梵語在它所控制的學校內成為第三語言。在這些學校中，梵語是5到8年級的選修課。對於隸屬於全印度印度中等教育委員會的學校也是這樣，特別是在官方語言是印地語的邦之中。唯一採用梵語的日報《蘇達爾摩》在印度邁索爾從1970年開始出版。自從1974年，在所有印度廣播電臺中都有了梵語的每日短新聞廣播。在一些印度村莊中，所有種姓的居民以梵語爲母語。\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'qwe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d212debcce47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpara\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparapraphs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpara\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'context'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mqweqwe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqwe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'qwe' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('data/dev-small.json') as f:\n",
    "    for article in json.load(f)['data']:\n",
    "        parapraphs = article['paragraphs']\n",
    "        for para in parapraphs:\n",
    "            print(para['context'])\n",
    "        qweqwe = qwe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [1,2,3,4,5]\n",
    "probs = [-1,-2,-3,1,2]\n",
    "all_predictions={}\n",
    "all_predictions.update(\n",
    "   {\n",
    "#         uid: 'answer' if prob < 0 else ''\n",
    "        uid:prob\n",
    "        for uid, prob in zip(ids, probs)\n",
    "   }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: -1, 2: -2, 3: -3, 4: 1, 5: 2}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a=np.array([[10,2,3,4],\n",
    "            [3,5,2,7],\n",
    "            [6,6,8,12]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 2]\n",
      "[3 1 0]\n",
      "[3 2 1]\n"
     ]
    }
   ],
   "source": [
    "for b in a:\n",
    "    print(b.argsort()[-3:][::-1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object <genexpr> at 0x7f7e04b5c050>\n"
     ]
    }
   ],
   "source": [
    "[b.argsort()[-3:][::-1] for b in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [0,3,2,1]\n",
    "np.arrayx[-3:][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
